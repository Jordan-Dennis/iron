\documentclass[a4paper, twocolumn]{article}
\setlength{\textwidth}{180mm}
\setlength{\textheight}{250mm}
\setlength{\parindent}{0mm}
\setlength{\parskip}{2mm}
\setlength{\oddsidemargin}{15mm}
\setlength{\hoffset}{-1in}
\setlength{\topmargin}{-2.5pc}
\setlength{\headsep}{20pt}
\setlength{\columnsep}{4mm}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\lstset{
    language=C, 
    literate={\ \ }{{\ }}1, 
    keywordstyle=\bfseries, 
    basicstyle=\ttfamily}
\def\ttdefault{pcr}
\begin{document}
\section*{Foreword}
I used the \verb!C! programming language to complete this task. I %
represented the system of spins as a \verb!struct!, %
defined below in one dimension. 

\begin{lstlisting}
typedef struct ising_t
{
    float epsilon;
    float magnetic_field;
    float epsilon;
    int length;
    int *ensemble;
} ising_t;
\end{lstlisting}

Scaling this to two dimensions was more difficult because the %
\verb!*ensemble! had to be transformed into \verb!**ensemble!. %
It is possible to implement this in a single \verb!struct! %
using \verb!union ensemble {int *1d, int **2d};!, but in the %
code available on my \verb!github! I have used two separate %
\verb!struct!s resulting in code duplication for many of the %
methods. I made this decision because I had not used \verb!c! %
prior to this project and was unaware of the \verb!union! %
keyword and its uses. I am describing this because you may %
notice discrepancies in the code snippets that I have include %
where \verb!int *(*)ensemble = system -> ensemble;! is invoked %
to access the array of spins. 


\verb!C! does not have native tooling for generating plots %
so I wrote the output of the simulations to \verb!.csv! files %
that I then plotted using \verb!python! and \verb!matplotlib!. %
I managed to get some of my \verb!C! code to run in parallel %
which was very cool but nevertheless, I ended up waiting for %
a very long time (\(\sim 9hrs\)) for some of the code to run. %
If I were writing the assessment again I would reduce the %
recommended size downward from one-hundred except when generating %
visuals in two dimensions. My \verb!github! is \verb!Jordan-Dennis!
and the repository is \verb!iron!; %
\verb!https://github.com/Jordan-Dennis/iron.!


\section*{Introduction}
The (initial) aim of my analysis is to construct a simple model, %
and simulation of a ferro-magnetic material. Ferro-magnetic materials %
behave as magnets below a certain temperature but like para-magnets %
above that temperature. A para-magnet can be thought of as an %
ensemble of spins which, in the absence of an external magnetic %
field have no tendency to align \cite{Schroder2021}. It follows %
that a ferro-magnetic material is characterised by interactions %
between the spins in the ensemble. 


Before diving into the model it is worth considering examples %
of real materials that have ferro-magnetic properties. The most %
obvious example is Iron, which is the name-sake. If you were %
to hold a lump of pure Iron in your hand it would almost certainly %
not exhibit no macroscopic magnetic properties. The temperature %
would not affect you perception of the magnetisation either. %
You could freeze or melt it and it would still seem magnetically %
inert. 


The explanation lies in the way the spins in iron influence each %
other. The effects are local so microscopic segments of Iron, %
which still correspond to billions of atoms, align themselves. %
The reason we do not necessarily observe this magnetic affect %
is because neighbouring sets of aligned spins called \emph{domains} %
do not necessarily align with each other. In fact often they %
destroy the magnetic affect of the neighbouring \emph{domains}. 


Heating the Iron we can destroy the domains. As we energise the %
sample the interactions between the spins becomes much less %
than the thermal energy and so they flip between states %
much more readily destroying the coherent structures. To %
make the magnetic affect permanent we can force all of the %
spins to align using an external magnetic field and then %
cool the Iron back down essentially freezing the spins in place. 


It is worth discussing this example because it informs us of the %
properties that we should expect to see in our model. Firstly, %
we are expecting temperature to play a role in the net magnetisation %
of the ensemble. Moreover, we want to observe \emph{domains} form %
below some temperature. These should be \emph{quasi-stable} %
meaning that they have a lifetime much longer than the lifetime %
of an individual spin. 


Our ferro-magnetic model is an implementation of the Ising model. %
The Ising model is widely used \cite{MacKay2003}, because of its %
inherent simplicity. The assumptions of this model are that %
every spin in the ensemble can either point upward or downward %
but not in any other direction. This is obviously untrue of %
nature, but we should still be able to see domains along the %
two orientations. The second assumption of the Ising model %
is that only nearest neighbours interact. 


The nearest neighbour assumption is probably fine since in nature %
long range forces tend to fall of with \(1 / r^{2}\) so the %
dominant force will be from the nearest neighbour. However, %
the term nearest neighbour implies a structure upon the model. %
In one dimensional it is relatively unambiguous but in two and %
three dimensions there are a lot of ways to arrange the spins %
in the ensemble and hence chose the nearest neighbours. How %
this selection impacts the outcomes is not a focus of this %
report. 


Without further ado I will introduce the expression for the %
energy in the absence of an external magnetic field, 
%
\begin{align}
    U &= -\epsilon\sum_{p(i,j)}s_{i}s_{j},
    \label{eqn:20}
\end{align}
%
where, \(i\) and \(j\) are integers specifying the locations of %
two spins, denoted \(s\), that are pairs, denoted (\(p(i, j)\). %
\(\epsilon\) has the units of energy and represents the strength %
of the interaction. For this analysis I work in units of \(\epsilon\). %
Equation \ref{eqn:20} allows us to compute the partition function %
of the system,
%
\begin{align}
    Z &= \sum_{k}\exp\left(-\frac{U_{k}}{\tau}\right)
    \label{eqn:21}
\end{align}
%
where \(\tau = kT\) is the temperature in units of energy and %
\(k\) denotes a specific configuration of the ensemble. There %
are \(2^{N}\) ways to order the lattice hence the summation %
can be over a very large number of states indeed. 


Since this sum is so large I have opted to use a monte-carlo %
sampling pattern. Rather than attempt to evaluate all the %
terms in the sum monte-carlo techniques attempt to find a %
representative sample by making weighted random guesses. In %
this case we weight our guess by the Boltzmann factor of the %
state. The details of this algorithm were provided to us %
without proof and so I will not attempt to explain the %
technique in more detail.


Equipped with this information I first simulated a one dimensional %
ensemble of spins and investigated its thermodynamic properties. %
I have included this analysis in sections labelled by question %
because I thought it would be easier to mark. Once I had analysed %
the one dimensional Ising model I extended the simulations into %
two dimensions, the analysis for which I have also provided %
in a by section format. I have discussed methods where I thought %
it fitted best with the question format and I have not cited the %
information that was provided with the cover letter. 


\subsection*{Question 1 a)}
I selected three different temperatures, \(1.0, 2.0\) and %
\(3.0 \epsilon / k\), and ran the metropolis algorithm for %
\(1000N\) steps. At each of these temperatures the system %
was initialised in a random state and allowed to equilibrate. 

\begin{figure}[h]
    \includegraphics%
        [width=0.45\textwidth]%
        {pub/figures/first_and_last_ising_1d.pdf}
    \caption{Each vertical pair of lines %
        represents the spin state. The top one is %
        the random initial state and the bottom one %
        is the equilibrated final state.}
    \label{fig:1}
\end{figure}

We noticed that at lower temperatures the spin chunks were %
much larger than at higher temperatures. This is particularly %
pronounced between the \(T = 3.0\epsilon/k\) and \(T = %
1.0\epsilon/k\) plots in figure \ref{fig:1}. To evolve the %
system we used the version of the metropolis algorithm %
shown below, 

\begin{lstlisting}
/*
 * metropolis_step
 * ------------------------ 
 * Evolve the system by attempting to 
 * flip a spin. The step is weighted 
 * by the Boltzmann factor.
 *
 * parameters
 * ----------
 * ising_t *system: A struct 
 *     enscapulating the information 
 *     related to the system. 
 */
void metropolis_step(ising_t *system)
{
    float temp = system -> temperature;
    int *ensemble = system -> ensemble;
    int length = system -> length;
    int spin = random_index(length);
    int change = 2*ensemble[spin]*(
        ensemble[modulo(spin+1,length)] +
        ensemble[modulo(spin-1,length)]);

    if ((change < 0)
        || (exp(-change/temp)>randn()))
    {
        ensemble[spin] *= -1;
    }
}
\end{lstlisting}

where, \verb!rand! generates a random number in the range %
\([0, 1]\), and \verb!modulo! is modified to produce %
positive input on negative numbers like the \verb!python! %
implementation. This is not the native \verb!C! implementation. %
I used the \verb!||! short circuit operator so that %
the second comparison was not evaluated on every call %
to the function. 
            

\subsection*{Question 1 b)}
I found that it was worth considering what the \emph{basic %
unit} of the Ising model was. In the absence of an external %
magnetic field the energy is a function of the pairs. I start %
by considering the partition function of an individual pair. %
This is a two level system; either the pair are aligned or they %
are anti-aligned with the corresponding energies.
%
\begin{align}
    Z_{i} &= \sum_{s_{i} = \pm 1}
            \exp\left(-\frac{\epsilon s_{i}s_{i+1}}{\tau}\right)
            \nonumber\\
        &= \exp\left(-\frac{\epsilon}{\tau}\right) +
            \exp\left(\frac{\epsilon}{\tau}\right)
            \nonumber\\
        &= 2\cosh\left(\frac{\epsilon}{\tau}\right).
    \label{eqn:1}
\end{align}


We can multiply the partition functions of single constituents%
together to get the partition function of the entire system. %
However, the condition to do this is that the constituents were %
independent, but the Ising model contains interactions. In the case %
of the Ising model the constituents that are independent are the %
pairs, not the individual spins. You may think think then that we %
only consider \(N / 2\) unique pairs but this is not the case. In a %
chain each spin is counted in two pairs so the power is still \(N\). 


A small detail that I skipped was what happens at the boundary. %
The two spins on the end of the chains are not (necessarily) %
counted twice. In the limit of a very large chain of spins we %
can see that the boundary affect will not matter however, we %
got about this nuance in a much more interesting way using %
cyclic boundary conditions. That is to say that the spin on the %
far end of the chain is a neighbour to the spin at the start of the %
chain and vice versa. 


Given the partition function \(Z = (2\cosh(\epsilon / \tau))^{N}\), we 
calculated the internal energy using,

\begin{align}
    U &= \tau^{2}\partial_{\tau}\ln(Z) \nonumber\\
        &= \tau^{2}\partial_{\tau}
            \ln\left(2\cosh\left(\frac{\epsilon}{\tau}\right)^{N}\right)
            \nonumber \\
        &= N\tau^{2}\partial_{\tau}
            \ln\left(2\cosh\left(\frac{\epsilon}{\tau}\right)\right)
            \nonumber \\
        &= N\tau^{2}\partial_{\tau}
            \left(2\cosh\left(\frac{\epsilon}{\tau}\right)\right)
            \frac{1}{2\cosh\left(\frac{\epsilon}{\tau}\right)}
            \nonumber \\
        &= N\tau^{2}\partial_{\tau}\left(\frac{\epsilon}{\tau}\right)
            \frac{\sinh\left(\frac{\epsilon}{\tau}\right)}
            {\cosh\left(\frac{\epsilon}{\tau}\right)}\nonumber \\
        &= -\epsilon N\tanh\left(\frac{\varepsilon}{\tau}\right)
    \label{eqn:2}.
\end{align}

We also calculated the free energy:

\begin{align}
    F &= -\tau\ln Z \nonumber \\
        &= -\tau\ln\left(\left(
            2\cosh\left(\frac{\epsilon}{\tau}\right)\right)^{N}\right) 
            \nonumber \\
        &= -N\tau\ln\left(
            2\cosh\left(\frac{\epsilon}{\tau}\right)\right) 
            \nonumber \\
        &= -N\tau\ln\left(
            \exp\left(\frac{\epsilon}{\tau}\right) + 
            \exp\left(-\frac{\epsilon}{\tau}\right)\right) \nonumber \\
        &= -N\tau\ln\left(\exp\left(\frac{\epsilon}{\tau}\right)
            \left(1 + \exp\left(-2\frac{\epsilon}{\tau}\right)\right)
            \right)\nonumber \\
        &= -N\tau\ln\left(\exp\left(\frac{\epsilon}{\tau}\right)\right)
            - N\tau\ln\left(1 + 
            \exp\left(-2\frac{\epsilon}{\tau}\right)\right) \nonumber \\
        &= -N\epsilon - N\tau\ln\left(1 + 
            \exp\left(-2\frac{\epsilon}{\tau}\right)\right)
    \label{eqn:3}. 
\end{align}

The entropy followed from the combination of Equation \ref{eqn:2} and %
Equation \ref{eqn:3},

\begin{align}
    \tau\sigma &= F - U \nonumber \\
        &= -N\epsilon\tanh\left(\frac{\epsilon}{\tau}\right) + 
            N\epsilon + N\tau\ln\left(1 + 
            \exp\left(-2\frac{\epsilon}{\tau}\right)\right)\nonumber \\
    \sigma &= \frac{\epsilon}{\tau}\left(1 - 
            \tanh\left(\frac{\epsilon}{\tau}\right)\right) +
            \ln\left(1 + \exp\left(-2\frac{\epsilon}{\tau}\right)\right)
    \label{eqn:4}.
\end{align} 

Finally, we determined the specific heat using Equation \ref{eqn:2} %
and Equation \ref{eqn:3},

\begin{align}
    C &= \partial_{\tau}U \nonumber \\
        &= \partial_{\tau}\left(-N\varepsilon\tanh\left(
            \frac{\varepsilon}{\tau}\right)\right)\nonumber\\
        &= -N\varepsilon\partial_{\tau}\left(\frac{\varepsilon}{\tau}\right)
            \frac{1}{\cosh^{2}\left(\frac{\varepsilon}{\tau}\right)}
            \nonumber\\
        &= \frac{N\varepsilon^{2}}{\tau^{2}\cosh^{2}\left(
            \frac{\varepsilon}{\tau}\right)}
    \label{eqn:5}.
\end{align}    


\subsection*{Question 1 c)}
\begin{figure}[h]
    \centering
    \includegraphics%
        [width=0.45\textwidth]%
        {pub/figures/physical_parameters_ising_1d.pdf}
    \caption{The top left is the energy and the %
        top right is the entropy. The bottom left is the %
        free energy and the bottom right is the heat capacity.}
    \label{fig:2}
\end{figure}        

Starting with our one dimensional model we equilibrated the %
system for multiple different temperatures and settled on %
using \(1000N\) as the length of the loop. This was likely %
too many but I found that for low temperatures when the %
probability of a flip becomes small, a larger number of %
steps was required. 


I chose to sample the temperatures over the range \(0.0 - 4.0 %
\epsilon / k\) incrementing by \(0.2 \epsilon / k\). I %
initialised the system only once at the highest temperature %
that we sampled, \(3.8 \epsilon / k\). I equilibrated the %
system at this temperature by evolving it for \(1000N\) %
and then started to cool the system taking measurements %
at each new temperature. 


The alternative model was to randomly initialise the system %
at every temperature. This would require approximately %
twice the number of steps since the system would have to %
be equilibrated at every temperature. I realize that the %
cooling method has a side affect of leading to "overflow". %
By "overflow" I mean that the first few measurements of %
each temperature are slightly out of equilibrium at the %
higher temperature. 


The entire \emph{measured} cooling process was performed %
by the program \(1000\) times. At each temperature in the %
\emph{measured cooling} loop the energy and entropy %
were measured by taking the average of all \(1000 N\) %
iterations. The heat capacity was also measured by taking %
the variance of the energy and applying,
%
\begin{equation}
    C_{v} = \frac{\textrm{var}(\epsilon)}{\tau^{2}}
    \label{eqn:6}
\end{equation}


The energy and entropy were re-averaged over the outer %
\emph{fixed size} loop. I chose to re-average then on %
the outer loop because I was certain that the trials were %
statistically independent. This does not matter so much %
for the mean value estimate since the \(1000N\) inner %
loop allows the system to explore the equilibrium space %
but I think that it does matter for the error estimate. 


I believe that it matters for the error estimate because %
the state of the system is highly correlated to the %
past state of the system within some \emph{correlation %
length}. This \emph{correlation length} is roughly the %
same amount of time that the system requires to explore %
the equilibrium state, or \(1000N\).


By running multiple simulations for the \emph{correlation %
length} and averaging these results I have guaranteed %
robust results. I estimated the error using the \emph{standard %
error} of the independent \emph{correlation length} simulations. %
This means that the error presented in figure \ref{fig:2} is %
given by equation \ref{eqn:7}.
%
\begin{equation}
    \Delta\hat{\beta} = \sqrt{\frac{\textrm{var}(\beta)}{N}},
    \label{eqn:7}
\end{equation}
%
where, \(\hat{\beta}\) is the parameter estimate, \(\beta\) is a %
vector of measurements and \(N\) is the number of measurements. 


To calculate the energy of the system I used the following %
algorithm, 

\begin{lstlisting}
/*
 * energy_ising_t
 * --------------
 * Calculate the energy of the system.
 *
 * parameters
 * ----------
 * ising_t *system: A struct 
 *     enscapulating the information 
 *     related to the system. 
 *
 * returns
 * -------
 * float energy: The energy of the
 *     system in Joules. 
 */
float energy_ising_t(ising_t *system)
{
    int length = system->length;
    int *ensemble = system->ensemble;
    float energy = 0.;

    for(int spin=0; spin<length; spin++)
    {
        energy -= ensemble[spin] *  
            ensemble[modulo(spin+1,length)];
    }

    return energy;
}
\end{lstlisting}


You may notice that I am only counting the right-hand neighbour %
of each spin. I chose this method because it is more optimal. %
If I was to count each neighbour then every pair would be %
counted twice and we would have to divide the final result by %
\verb!2.!. By only counting one of the neighbours I have %
halved the number of computations. 


A pair of spins is the \emph{base unit} of the Ising model %
so to calculate the entropy I counted the number of aligned %
pairs and then used the \emph{chose} function to calculate %
the multiplicity. However, it was not quite this simples %
since \(100!\) is \(\sim 10^{157}\), which overflows an %
integer in the programs memory. 


To fix this problem I used the Stirling approximation to %
compute the entropy directly. This implies that the %
entropy should be less accurate at lower temperatures %
where the entropy is low and the Stirling approximation %
diverges from the actual entropy. The code that I used %
to calculate the entropy was,

\begin{lstlisting}
/*
 * entropy_ising_t
 * ---------------
 * Calculate the entropy of a 
 * configuration. 
 *
 * parameters
 * ----------
 * ising_t *system: A struct 
 *     enscapulating the information 
 *     related to the system. 
 *
 * returns
 * -------
 * float entropy: The entropy of the
 *     system in natural units. 
 */
float entropy_ising_t(ising_t *system)
{ 
    int length = system->length;
    int *ensemble = system->ensemble; 
    int up = 0;

    for (int spin=0; spin<length; spin++)
    {
        up += ensemble[spin] == 
            ensemble[modulo(spin+1,length)];
    }

    int down = length - up;

    float entropy = length * log(length) - 
        up * log(up) - down * log(down);
    
    return entropy;   
}
\end{lstlisting}


\subsection*{Question 1 d)}
Consider the energy depicted in figure \ref{fig:2}. We see that %
the energy is a decreasing function of the temperature. This %
implies that the spins tend to align as the temperature decreases. %
This makes sense because the Boltzmann factor for the lower state %
becomes more favoured. Similarly the entropy is a increasing %
function of the temperature. As the spins tend to align at lower %
temperatures the number of ways to arrange the state becomes %
smaller, decreasing the entropy. 


The heat capacity, also shown %
in figure \ref{fig:2} is interesting because it increases %
rapidly before asymptotically decreasing. If it was discontinuous %
then we would expect a phase transition however it is continuous. %
so we know that there is no phase transition. We know it is continuous %
because of equation \ref{eqn:5}. 


\subsection*{Question 1 e)}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{pub/figures/magnetisation_ising_1d.pdf}
    \caption{Histograms of the magnetisation for two ising %
        systems of sizes \(N = 100\) and \(N = 500\). The %
        temperatures are labelled in the titles and the %
        \(y\)-axis is a unitless count.}
    \label{fig:3}
\end{figure}


Figure \ref{fig:3} showed me that the magnetisation was not zero %
as it was predicted to be at \(0 \epsilon / k\). I am not concerned %
because the distribution of samples, which roughly corresponds to %
a time average is symmetrically distributed around zero implying %
that the time average is identically zero. I would expect some %
variation of the results around zero as spins randomly flipped. %


Since magnetisation sets in at a lower temperature for the larger %
sample I concluded that there was no phase transition. I %
decided this because the phase transition should occur at the %
same temperature for all lattice sizes. Taking the limit of %
a decreasing function presumably reduces to zero implying that %
there is no spontaneous magnetisation for the one-dimensional %
Ising model. This means that there is no phase transition. 


\subsection*{Question 2 a)}
For \(\tau = 1.0 \epsilon / k\) the random lattice very quickly %
relaxed into large \emph{domains} of up and down spins. Over time %
the boundaries of the \emph{domains} moved around the lattice %
coherently. I am/will using/use \emph{coherent motion} to refer %
to a blob transporting across the network. For example, a group %
of down spins may start in the top right corner and remaining %
a single blob move to the middle of the system. 


I noticed that given enough time the \(\tau = 1.0 \epsilon / k\) lattice %
relaxed until it was entirely one color. This is not shown in figure %
\ref{fig:4} simply because the lattice was not allowed to evolve %
for enough time. I used the same evolution time for the two %
dimensional Ising model that I used for the one dimensional %
Ising model, that is \(1000N^{2}\) where I will use \(N\) to %
represent the side length of the system. 


This is important because at lower temperatures the system took longer %
to equilibrate. As I saw for the one-dimensional case the %
\emph{coherent domains} have a \emph{meta-stable} lifetime before decaying. %
The shape of this domain influences the lifetime. The size %
seems to be a direct predictor of lifetime with the largest %
domain nearly always coming out on top. Another important %
feature that we noticed was that the coherent domains became %
particularly \emph{meta-stable} if they formed rings. 


The ring structures were \emph{meta-stable} because of the cyclic %
boundary conditions. In a ring structure there are only two edges %
that are exposed to the other \emph{meta-stable} state meaning %
that the anti-aligned interactions at the boundary are minimised. %
Additionally since the interaction depth of the Ising model is %
only nearest neighbour the \emph{meta-stable} ring could exit %
in a band of just five spins. \emph{The spins on the boundary %
have no idea how large their respective domains are}.


A final point to address for the \(\tau = 1.0\epsilon / k\) model %
is the \emph{time speckling}. As the system is allowed to evolve %
in equilibrium spins at random locations will randomly flip. %
This is a byproduct of the metropolis algorithm but it is important %
because the \emph{speckles} almost always remain a single spin. %
You will see later that this changes as the temperature is increased. 


Heating the lattice to \(\tau = 2.0 \epsilon / k\) we see mush %
of the same behaviour. \(\tau = 2.0 \epsilon / k\) is still less %
than the critical temperature, so all of the same evolutionary %
events occur. \emph{Meta-stable} domains form, move and die. %
Even the more dangerous \emph{meta-stable} rings can form at %
this temperature. I would like to say that the lifetime of the %
\emph{domains} has declined but I have no evidence. 


The key difference between the \(\tau  2.0 \epsilon / k\) and the %
\(\tau = 1.0 \epsilon / k\) models is the \emph{speckling}. At %
the higher temperatures the speckles occur more often and can %
result in very local short lived domains. This is why I postulated %
that the \emph{domains} are shorter lived. The system is more %
active meaning that the spins on the boundary flip more readily. %
Moreover, the rapid speckling assists in the destruction of the %
\emph{domain} when it occurs on the boundary.


Something worthy of note before we move above the condensation %
temperature is that \emph{domains} die at there corners. This %
is because the corners represent an interface where a spin has %
more neighbours of one color than the other. This is another %
factor that prolongs the life of the \emph{meta-stable} states, %
since they have no corners and minimised contact. 


The behaviour was markedly different for the \(\tau = 3.0 \epsilon %
\ k\) model, because it is above the critical temperature. %
For starters the activity of the system has increased significantly. %
This means, among other things that the relaxation into equilibrium %
is almost instantaneous compared with the lower temperature models. %
In addition \emph{domains} do not form at this temperature. 


I have been using \emph{domain} to refer to a structure that is %
\emph{macroscopic} and \emph{semi-permanent}. \emph{Zonation} %
does occur at \(\tau = 3.0 \epsilon / k\) bur it does not meet %
these criteria. By this I mean that the zones are small \(\sim 10\) %
spins maximum and temporary. Rather than considering the rapid %
zonation to be the evolution of the \emph{domain} behaviour I %
consider it the evolution of the \emph{speckling} behaviour.


At \(\tau = 3.0 \epsilon / k\) we also loose the \emph{coherent} %
end state. At both of the other temperatures the final state was %
a completely magnetised block or a \emph{meta-stable} ring. %
However, at \(\tau = 3.0 \epsilon / k\) there is no such state %
and the \emph{zonation} appears to randomly move about the grid. %
At higher temperatures still the \emph{zonation} is completely %
destroyed and not even microscopic domains are able to form. 


\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{pub/figures/first_and_last_ising_2d.pdf}
    \caption{Snapshots of the Ising system at multiple temperatures %
        given a random initial starting position (top). You will %
        notice that \(\tau = 1.0 \epsilon / k\) and \(\tau = 2.0 \epsilon %
        /2\) have similar net magnetisation, but that \(\tau = 2.0 %
        \epsilon / k\) has stronger \emph{speckling} than \(\tau = %
        1.0 \epsilon / k\). You will notice that there is not net %
        magnetisation for \(\tau = 3.0 \epsilon / k\) and that the %
        \emph{zonation} I described above is occurring.}
    \label{fig:5}
\end{figure}


\subsection*{Question 2 b)}
Smoothly varying the temperature using I found that the \emph{speckling} %
and \emph{zonation} seems to dominate above \(\sim 2.3 \epsilon / k\) %
while below this \emph{domains} start to form and given enough time %
the entire crystal becomes magnetised. The transition is smooth %
with respect to the \emph{speckles}, which at very low temperatures %
are very small and isolated with out a lifetime. 


As I increased the temperature the \emph{speckles} increases in size and %
in lifetime. A \emph{speckle} is differentiated from a \emph{zone} rather %
arbitrarily, the two main differences being transport and lifetime. The %
\emph{zones} have a lifetime of long enough that they are able to %
\emph{coherently move} across the crystal. On the other hand a \emph{speckle} %
tends to die where it started and very quickly. 


As we approach the critical temperature from below the \emph{speckles} %
increase in size and disrupt the domains more quickly but, the system %
is still able to fully magnetise. At the critical temperature the %
\emph{speckles} become so disruptive that the magnetisation cannot occur %
macroscopically but it still occurs locally in the form of the \emph{zones} %
which outlive the now very active \emph{speckles}. 


Increasing the temperature further still the coherent \emph{zones} %
disappear (visually) entirely and the speckles become the dominant %
behaviour. As a final note it is worth noting that these behaviours %
I have described were visible to me in part because of the settings %
of my simulation. I found it was best to run them with many steps %
per frame \(5000 \sim 10000\) because it allows you to see the %
macroscopic behaviours. 


I acknowledge that although I wanted to use the \verb!qt! frame work %
to create my own \verb!gui! I ran out of time. Instead I used the %
free online applet created by Daniel V. Schroeder for Weber state %
university to make the higher level observations. The code that I used %
to evolve the two dimensional system was, 

\begin{lstlisting}
/*
 * metropolis_step_ising_t
 * -----------------------
 * Evolve the system according to a 
 * randomly weighted spin flip that 
 * compares the probability of the 
 * two states based on the Boltzmann 
 * distribution of the two systems. 
 *
 * parameters
 * ----------
 * ising_t *system: The system to 
 *     evolve. 
 */
void metropolis_step(ising_t *system)
{
    int length = system->length;
    int **ensemble = system->ensemble;
    float epsilon = system->epsilon;
    float temp = system->temperature;
    float field = system->magnetic_field;

    int row = random_index(length);
    int col = random_index(length);

    int spin = ensemble[row][col];
    int neighbours = 
        ensemble[modulo(row+1,length)][col] + 
        ensemble[modulo(row-1,length)][col] + 
        ensemble[row][modulo(col+1,length)] + 
        ensemble[row][modulo(col-1,length)];

    float magnetic_change=-2*spin*field;
    float interaction_change=2*epsilon*
        neighbours*spin;
    float change=magnetic_change+
        interaction_change;

    if ((energy_change < 0) || 
        (exp(-change/temperature) > randn())))
    {
        ensemble[row][col] *= -1;
    }
}
\end{lstlisting}


\textbf{Note}: The \verb!magnetic_field! was set to zero in the %
provided simulation and discussion, while the \verb!epsilon! %
was set to one. 


\subsection*{Question 2 c)}
\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{pub/figures/physical_parameters_ising_2d.pdf}
    \caption{}
    \label{fig:7}
\end{figure}
I noticed that the heat capacity \emph{spike} increased in %
height as I increased the size of the crystal. This suggests %
that the infinite Ising model will have an infinitely tall %
heat capacity \emph{spike} at the critical temperature. I also %
noticed that at the critical temperature all of the physical %
parameters changed the fastest. 


This makes sense because a phase transition is loosely defined %
as a sudden discontinuity in the bulk properties of the sample. %
Although the entropy and energy are not discontinuous over the %
the critical temperature they do change very suddenly. The %
discontinuity is in the heat capacity which is the derivative %
of the energy with respect to temperature. Because a derivative %
is the discontinuous quantity it implies that the phase transition %
is second order. 


To measure the energy of the two dimensional Ising system I %
used a very similar piece of code to the one dimensional %
scenario. 

\begin{lstlisting}
/*
 * energy_ising_t
 * --------------
 * Calculate the energy of the 
 * Ising system. 
 *
 * parameters
 * ----------
 * ising_t *system: The system to 
 *     measure. 
 */
float energy(ising_t *system)
{
    int length = system->length;
    int **ensemble = system->ensemble;
    float epsilon = system->epsilon;
    float field = system->magnetic_field;
    float magnetic = 0.0;
    float interactions = 0.0;

    for (int row=0; row<length; row++)
    {
        for (int col=0; col<length; col++)
        {
            float neighbours = 
                ensemble[modulo(row+1,length)][col] +
                ensemble[modulo(row-1,length)][col] +
                ensemble[row][modulo(col+1,length)] +
                ensemble[row][modulo(col-1,length)];

            magnetic -= ensemble[row][col] * 
                field;
            interactions -= neighbours * 
                epsilon * ensemble[row][col];
        }
    }

    return interactions / 2. + magnetic;
}
\end{lstlisting}

For the entropy the calculation was also very similar. 

\begin{lstlisting}
/*
 * entropy_ising_t
 * --------------
 * Calculate the entropy of the Ising 
 * system. 
 *
 * parameters
 * ----------
 * ising_t *system: The system to 
 *     measure. 
 */
float entropy_ising_t(ising_t *system)
{
    int len = system->length;
    int **ensemble = system->ensemble; 
    int up = 0;

    for (int row=0; row<len; row++)
    {
        for (int col=0; col<len; col++)
        {
            up += ensemble[row][col] == 
                ensemble[modulo(row+1,len)][col];
            up += ensemble[row][col] == 
                ensemble[row][modulo(col+1,len)];
        }
    }
        
    int total = 2 * len * len;
    int down = total - up;

    return total * log(total) - 
        up * log(up) - down * log(down);
}
\end{lstlisting}


\subsection*{Question 2 d)}
\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{pub/figures/magnetisation_ising_2d.pdf}
    \caption{The magnetisation of the Ising model as the %
        temperature and number of spins is varied.}
    \label{fig:8}
\end{figure}


I noticed that the magnetisation began to diverge at the same %
temperature irrespective of the size of the system. This is what %
I would expect from a phase transition because it should not depend %
on the size of the sample. To give a real world example iron melts %
at the same temperature regardless of the size of the block. %
We might more quickly melt a smaller block but that is just because %
there is less of it and less self insulation. 


As \(N\) increases the critical temperature will stay the same, %
but the steepness of the transition appears to radically increase. %
I was surprised to see this relationship and I believe it may %
be because we are at the very low number end of the spectrum. %
In this regime if one or two spins flip it becomes visible on %
the graph. What I expect to see is that the lines smoothly %
converge down to a step function at the critical temperature. 


\subsection*{Question 2 e)}
\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{pub/figures/heating_and_cooling_ising_2d.pdf}
    \caption{Heating and cooling an Ising system to change the %
        magnetisation. The leftmost figure is the initial state %
        at the lowest temperature, the middle figure is the highest %
        temperature state and the rightmost figure is the final %
        final state after the system has again been cooled.}
    \label{fig:9}
\end{figure}

I initialised the system randomly. This corresponds to the %
equilibrium of a system that is at a high temperature. %
Therefore, I first cooled the system to reach an equilibrium %
that I would expect for a low temperature state. This was done %
slowly letting the system equilibrate at each new temperature. %
This technique avoids the \emph{meta-stable} rings from forming. 


The cooled system represents the leftmost figure in \ref{fig:9}. %
We can see that the magnetisation is complete and in this case %
there are not even any speckles. This is suspiciously good but %
I cannot find the error in my code so I will run with it. At %
the highest temperature we see that the state appears to be %
almost entirely random. Cooling it back down I managed to get %
a \emph{meta-stable} ring state.


In all the heating and cooling operations I raised the temperature %
slowly and re-equilibrated the system at each new temperature. %
I am annoyed by the ring state because this gradual cooling is %
supposed to prevent such states for the most part. Again this %
state is markedly non-speckled and I think this is because the %
temperature is very close to \(0.0 \epsilon / k\). We also see %
that the edges of the ring are pixel flat. 


I justify this because the corners are where the \emph{domains} %
die as I discussed above. Hence any incursion that yellow makes %
into the purple band is unstable because the incurring yellow %
are more likely to have purple neighbours than yellow neighbours. %
The same can be said for the purple incursions into yellow %
territory. Therefore, by far the most stable configuration is %
the flat edged band which we observe. 


The final thing I want to discuss is the change in the magnetisation %
of the system between the first cool state and the final cool state. %
We can see that the injection of thermal energy into the system %
has changed the magnetisation from purple to predominantly yellow %
ignoring the \emph{meta-stable} ring. 


The Currie temperature is defined as the temperature at which the %
magnetic properties of a metal change dramatically. Since this is %
the behaviour that we are observing at the critical temperature %
it makes sense that the critical temperature is analogous to the %
Curie temperature. In the briefing %
we were given that the critical temperature for the two dimensional %
Ising model was \(T_{c} \approx 2.27\epsilon / k\). The Curie %
temperature of iron is \(T_{c} = 1043K\), hence \(\epsilon = %
1043K * k / 2.27 = 0.0396eV\). 
\vfill


\clearpage
\section*{Introduction}
The study of magnetic materials is an area of academic %
and industrial interest \cite{Shroeder2021}. For example, magnetic %
technologies are important in the ongoing development of %
quantum computers, superconducting circuits and other %
examples in electronics. At a fundamental level %
magnetisation is a well understood phenomenon, yet it is %
difficult to theoretically model. One simple model of magnetic %
materials is the Ising model. 


The Ising model is a simple model of a ferro-magnet \cite{MacKay2003}. %
Despite the simplicity of the Ising model it displays rich %
physical behaviour and has analytic solutions in one and %
two dimensions \cite{MacKay2003, Schroeder2021}. The Ising model is the %
simplest model to account for inter-molecular interactions and contain a phase %
transition. This makes it an excellent medium for studying %
magnetic phenomenon \cite{MacKay2003}. 


By modifying the basic Ising model we can simulate many %
phenomenon including glasses \cite{McMillan1984}. The Ising model %
has broader significance and can be used to construct very %
simple neural networks called Boltzmann machines \cite{MacKay2003}. We %
used the Ising model to compare ferro-magnetic, para-magnetic and %
anti-ferro-magnetic materials. Iron is a good example of a material %
that is ferro-magnetic, whereas gold is para-magnetic and chromium %
is anti-para-magnetic. 


\section*{Theory}
Materials have internal interactions. As physicists we like %
to ignore these where possible but often these approximations %
limit the accuracies of our models \cite{Schroeder2021, Kittel1969}. Magnetic %
phenomenon are no different. To understand how spins interact %
in a magnet it helps to first construct the simplest possible %
model without interactions; a para-magnet.


Consider our magnet as a one-dimensional chain of atomic spins. %
For the moment ignore any external magnetic field and just %
consider the spins in isolation. Now lets limit the spins to %
be fixed up or down along one axis. If there are no interactions %
between the spins the energy is fixed. If we add an external %
magnetic field then we would expect the ensemble to develop a %
net magnetisation.


If the system has thermal energy we would expect some of the %
spins to align themselves anti-parallel to the magnetic field. %
We can see this affect by considering the partition function %
for a single spin in the ensemble. If the spin is aligned with %
the magnetic field then the energy is \(-sB\), where \(s\) is %
the unit of magnetisation carried by the single spin and \(B\) %
is the strength of the external magnetic field. If the spin is %
anti-aligned with the field then the energy is \(sB\). 


This is a simple two level system and the partition function %
is given by, 
%
\begin{align}
    Z &= \sum_{s = \pm 1}\exp\left(-\frac{sB}{\tau}\right)\nonumber\\
        &= \exp\left(-\frac{sB}{\tau}\right) + 
            \exp\left(\frac{sB}{\tau}\right)\nonumber\\
        &= 2\cosh\left(\frac{sB}{\tau}\right),
    \label{eqn:18}
\end{align}
%
where, \(\tau = kT\) is the temperature in units of energy. %
The probability of the spins being anti-aligned with the %
field is therefore, 
%
\begin{align}
    P &= \frac{\exp\left(-\frac{sB}{\tau}\right)}
            {2\cosh\left(\frac{sB}{\tau}\right)}.
    \label{eqn:9}
\end{align}
%
Hence, as the temperature increase we expect the number of %
anti-aligned spins to increase and as we increase the %
magnetic field we expect the number of anti-aligned spins %
to decrease. 


Since each of the spins in a para-magnetic system is independent %
the partition function of an ensemble of \(N\) spins is just %
the product of \(N\) partition functions for the single spin %
case. However, since the spins are indistinguishable we must %
also divide by a Gibbs correction factor of \(N!\). %
The energy of the system, and any other physical %
parameters, only depend on the number of spins that are %
aligned with the magnetic field and not specifically %
which spins are aligned with the field. 


It is informative to calculate the internal energy and free %
energy of the system. Starting with the internal energy,
%
\begin{align}
    U &= \tau^{2}\partial_{\tau}\ln Z\nonumber\\
        &= \tau^{2}\partial_{\tau}\ln\left(2^{N}\cosh^{N}
            \left(\frac{sB}{\tau}\right)\right)\nonumber\\
        &= -NsB\tanh\left(\frac{sB}{\tau}\right).
    \label{eqn:10}
\end{align}
%
We can also calculate the free energy, but further calculations %
result in tedious analytical expressions so we have omitted them. %
%
\begin{align}
    F &= -\tau\ln Z\nonumber\\
        &= -\tau\ln\left(2^{N}\cosh^{N}\left(\frac{sB}{\tau}
            \right)\right)\nonumber\\
        &= -NsB - N\tau\ln\left(1 + \exp\left(-\frac{2sB}{\tau}\right)
            \right).
    \label{eqn:11}
\end{align}
%
Curiously this matches with the results we found for the Ising %
model when there was no magnetic field. Using %
equation \ref{eqn:5} we can calculate the magnetisation as a function %
of the magnetic field and temperature, 
%
\begin{align}
    U = mB &= -NsB\tanh\left(\frac{sB}{\tau}\right)\nonumber\\
        m &= -Ns\tanh\left(\frac{sB}{\tau}\right).
    \label{eqn:12}
\end{align}
%
Therefore, the net magnetisation system will decrease with %
temperature and increase with the magnetic field, much as we %
would expect. 


Using equations \ref{eqn:12} and \ref{eqn:11} we can calculate the %
energy. Since the math is essentially the same as in the derivation %
of equation \ref{eqn:7} I have omitted some steps. 
%
\begin{align}
    \tau\sigma &= F - U \nonumber \\ 
        &= -Ns\left(B + \tanh\left(\frac{sB}{\tau}\right)\right) - \nonumber \\ 
        &\quad N\tau\ln\left(1 + \exp\left(-\frac{2sB}{\tau}\right)\right) 
    \label{eqn:13}
\end{align}


Para-magnets are a useful toy model but from our experience %
with natural and manufactured magnets we know that it is %
possibe to construct systems that are magnetic without external %
fields. The one-dimensional Ising model is a simple model of %
such systems. The Ising model is a natural extension of the %
paramagnetic model that we discussed, and operates on the same %
spin lattice. 


The Ising model differs because it adds very simple interactions %
between neighbouring spins. This interaction favours pairs that %
are aligned by reducing the energy of this scenario. Representing %
up spins as \(+1\) and down spins as \(-1\) we can represent this %
mutual interaction as \(\Delta U = \epsilon s_{i}s_{i + 1}\), %
where \(\Delta U\) is the energy contribution of the %
interaction, \(\epsilon\) is a scaling factor that represents %
the strength of the interaction and \(s_{i}\) is the \(i^{th}\) %
spin in the chain. 


What happens if we place the Ising model into an external magnetic %
field. Again we can break it down by considering a single pair %
in the chain as our constituent object. There are three energies %
that it is possible for this pair to have; parallel and aligned %
with the magnetic field, parallel and anti-aligned with the %
magnetic field and anti-parallel. However, the final state has a %
multiplicity of two since either of the spins could be aligned %
with the field. 


Based on our analysis of the paramagnetic system we would expect %
that the magnetic field would coerce spins into alignment by %
decreasing there energy. This produces a positive reinforcement %
loop because now the energy of the neighbouring spin is not %
only decreased by \(sB\) when it aligns with the field but is %
also decreased by \(\epsilon s_{i}s_{i + 1}\). We predict that %
adding the external field will raise the critical temperature of %
the system. 


We expect the type of phase transition to change. In %
general the spins will be flipping much less rapidly and as %
a result the humpy in the heat capacity will be smoothed out %
because the variance in the temperature will not peak as %
largely. Since, we expect that the heat capacity will be %
continuous this implies that the type of phase transition %
is now first order, where the energy is getting provided by the %
magnetic field. 


Moreover, we expect that many of the rich behaviours we observed %
in the absence of an external magnetic field will vanish. For %
example, below the critical temperature we expect the lifetime %
of \emph{domains} to be significantly shorter as the entire %
crystal quickly condenses into the ground state. In addition, %
we expect \emph{speckling} to be even rarer and slower because %
the flip now has a weight lower be a factor of \(\exp(-sB)\). 


Another interesting effect that can be explored using the Ising model %
is anti-ferro-magnetism. This phenomenon was only recently discovered %
in nature and refers to and interaction between neighbouring %
spins that causes them to have lower energy when they are aligned %
anti-parallel rather than parallel. We do not need to cover any new %
equations in this case as an anti-ferro-magnet can be explored by %
letting \(\epsilon\) become negative. 


It is worth noting that in the presence of a magnetic field the %
qualitative behaviour of the anti-ferro-magnetic and ferro-magnetic %
Ising models becomes markedly different. The ferro-magnetic model %
results in a positive feedback loop as the magnetic field coerces %
spins to align with the field they also want to align with each other.
On the other hand the anti-ferro-magnet exerts a dampening effect %
for the opposite reason. 


\section*{Hypothesis}
We predicted that the anti-ferro-magnetic material would have a phase %
transition at the same temperature as the ferro-magnetic material in %
the absence of an external magnetic field. We came to this conclusion %
because the type of interaction is the same and all that has changed is %
the sign. The presence of an external field was expected to change %
this phase transition, which we expected to be second order based on %
the ferro-magnetic Ising model, to a first order phase transition. 


We also predicted that the phase transition of the ferro-magnetic %
Ising model would change from a second order phase transition to a first %
order phase transition in the presence of an external magnetic field. %
This prediction was based on the unequal weighting of the orientations %
reducing the variability of the energy and hence smoothing out the %
peak in the heat capacity. A corollary of this second hypothesis was %
that the magnetic field would increase the critical temperature of %
the model. 


Finally we presented a number of equations predicting how the %
para-magnetic material would evolve as the magnetic field was varied. %


\section*{Method}
We tested our the anti-ferro-magnetic phase transition hypothesis by %
measuring the physical parameters at a number of different temperatures %
and plotting the results. This allowed us to search for a suspected %
discontinuity in the heat capacity which would indicate a second order %
phase transition akin to the phase transition of the ferro-magnetic %
substance. We repeated the same thing at three different strengths of %
magnetic field noting how the peak in the heat capacity qualitatively %
changed. As a result we were able to estimate the Neel temperature of %
the lattice based on the location of the \(B = 0\) peak.


We used the same method to test how the ferro-magnetic phase transition %
was changed by the external magnetic field. However, we also tested %
increasing the strength of the magnetic field once the system was %
equilibrated below above and at the critical temperature. This gave %
us the ability to look for a similar, but not the same phase transition %
which we expected to be first order. 


To test our predictions for the paramagnetic substance we simulated it %
over a fine grid of temperatures and the same three magnetic fields as %
the other simulations. We quantitatively assessed our models by calculating %
the \(\chi^{2}\) statistics for each fit and the associated \(p\)-value. %


My original code could not be used to test these predictions so I modified %
it to accommodate the flexible \(\epsilon\) and \(B\) into the sub-routines. %
Below I have included snapshots for the energy subroutine, entropy subroutine %
and the monte-carlo step subroutine. 

\begin{lstlisting}
/*
 * metropolis_step_ising_t
 * -----------------------
 * Evolve the system according to a 
 * randomly weighted spin flip that 
 * compares the probability of the 
 * two states based on the Boltzmann 
 * distribution of the two systems. 
 *
 * parameters
 * ----------
 * ising_t *system: The system to 
 *     evolve. 
 */
void metropolis_step(ising_t *system)
{
    int length=system->length;
    int **ensemble=system->ensemble;
    float epsilon=system->epsilon;
    float temp=system->temperature;
    float field=system->magnetic_field;

    int row = random_index(length);
    int col = random_index(length);

    int spin = ensemble[row][col];
    int neighbours = 
        ensemble[modulo(row+1,length)][col]+
        ensemble[modulo(row-1,length)][col]+
        ensemble[row][modulo(col+1,length)]+
        ensemble[row][modulo(col-1,length)];

    float magnetic=-2*spin*field;
    float interaction=2*epsilon*
        neighbours*spin;
    float change=magnetic+interaction;

    if ((change<0) || 
        (exp(-change/temp)>randn()))
    {
        ensemble[row][col]*=-1;
    }
}
\end{lstlisting}


\begin{lstlisting}
/*
 * energy_ising_t
 * --------------
 * Calculate the energy of the Ising 
 * system. 
 *
 * parameters
 * ----------
 * ising_t *system: The system to measure. 
 */
float energy_ising_t(ising_t *system)
{
    int len=system->length;
    int **ensemble=system->ensemble;
    float epsilon=system->epsilon;
    float field=system->magnetic_field;
    float magnetic=0.0;
    float interactions=0.0;

    for(int row=0;row<len;row++)
    {
        for(int col=0;col<len;col++)
        {
            float neighbours = 0.0;
            neighbours+=
                ensemble[modulo(row+1,len)][col]+
                ensemble[modulo(row-1,len)][col]+
                ensemble[row][modulo(col+1,len)]+
                ensemble[row][modulo(col-1,len)];

            magnetic+=ensemble[row][col]*field;
            interactions-=neighbours*epsilon*
                ensemble[row][col];
        }
    }

    return interactions / 2. + magnetic;
}
\end{lstlisting}


\begin{lstlisting}
/*
 * entropy_ising_t
 * --------------
 * Calculate the entropy of the Ising 
 * system. 
 *
 * parameters
 * ----------
 * ising_t *system: The system to 
 *     measure. 
 */
float entropy_ising_t(ising_t *system)
{
    int len=system->length;
    int **ensemble=system->ensemble;
    int up=0;

    for(int row=0;row<len;row++)
    {
        for(int col=0;col<len;col++)
        {
            up += 
                ensemble[row][col]==
                ensemble[modulo(row+1,len)][col]+
                ensemble[row][col]==
                ensemble[row][modulo(col+1,len)];
        }
    }
        
    int total=2*len*len;
    int down=total-up;

    return total*log(total)-up*log(up)-
        down*log(down);
}
\end{lstlisting}

However, the entropy subroutine that I have provided above does not %
work for the case \(\epsilon = 0\). This is because the base unit for %
the paramagnet is not a pair but a single spin. In this case we %
need to change how we count the number of up pairs/spins to actually %
count the number of up spins. I achieved this by changing the contents %
of the nested \verb!for! loop to \verb!up += ensemble[row][col] > 0!. %
The two different entropy calculations added the minor detail that %
the program had to chose/know which version of the function to apply. %
I will not bore you with the details, but I used function pointers %
to perform this task. 

\section*{Results}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{pub/figures/external_field_epsilon_minus_one.pdf}
    \caption{Snapshots of the evolution of the anti-ferro-magnetic %
        material at three different temperatures and three different %
        strengths of magnetic field.}
    \label{fig:8}
\end{figure}


It is informative to make some qualitative notes on the behaviour of the %
anti-ferro-magnetic material. I noticed that most of the behaviours %
we observed in the ferro-magnetic material could also be observed %
in the anti-ferro-magnetic material. One of my favourites was the %
\emph{domains} that formed. A small one is visible near the top left %
edge of the bottom left plot in figure \ref{fig:8}. These are %
characterised by two (or more) regions of chest-board spins separated %
by a line of aligned spins.


A key difference between the anti-ferro-magnetic material and the %
ferro-magnetic material is that the former has only one possible %
species. This may call into question my discussion of the \emph{domains} %
however, these are formed where the chest-board patterns are %
anti-aligned within two neighbouring regions (see figure \ref{fig:8}). %
This also calls into question what actually happens at the phase transition, %
since the net magnetisation is practically always zero. 


Varying the temperature we noticed that the size of the \emph{domains} %
reduced as we increased the temperature and there seemed to be a %
temperature around \(2.0 \epsilon / k\) where the size of a \emph{zone} %
was became comparable to the neighbourhood of a single spin. %
Hence we postulated that the phase transition was from anti-ferro-magnetism %
to paramagnetism. Our logic was that once the \emph{zones} were this %
small the net impact on any one spin essentially cancelled.


Adding the magnetic field had all of the impacts that we might have %
expected from the ferro-magnetic material. The size of the \emph{zones} %
around the critical temperature reduced and \emph{speckling} became %
much faster for spins aligned with the magnetic field, but slower for %
spins anti-aligned with the magnetic field. If purple represents the %
aligned spins then purple speckles would rapidly appear in the crystal %
before been overcome, while yellow speckles were almost never observed. %
We can very clearly see this affect on figure \ref{fig:8} along the top %
row. 


Interestingly, even at the greatest magnetic field we tests \(B = 3.0T\) %
we the size of the \emph{domains} in the low temperature state remained %
approximately the same. This makes sense as the states on either side %
of the domain wall remain equally favoured. Examining figure \ref{fig:8} %
we can see that there is an analouge to the \emph{meta-stable} states %
we encountered in the ferr-magnetic case. That is a ring that has formed. %
For the anti-ferromagnetic case all \emph{domains} appeared to be %
\emph{meta-stable} in our simulations. 


\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{pub/figures/external_field_epsilon_zero.pdf}
    \caption{Snapshots of the paramagnetic material equilibrated at %
        a variety of temperatures and magentic fields.}
    \label{fig:9}
\end{figure}


The para-magnetic material goes out of its way to be boring. There is only %
one behaviour and that is \emph{speckling}. The temperature tunes the %
rate that the \emph{speckling} occurs while magnetic field causes the %
one color to dominate the sample in a very linear fashion. At the strongest %
magnetic field the number of spins anti-aligned was almost none one %
the temperature was below \(2.0 \epsilon / k\) as shown in figure \ref{fig:9}.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.45\textwidth]{pub/figures/external_field_epsilon_one.pdf}
    \caption{Snapshots of a ferro-magnetic material for multiple %
        different magnetic field strengths and temperatures. For a %
        discussion of the behaviours I invite you to review my earlier %
        work.}
    \label{fig:10}
\end{figure}


\begin{figure*}
    \centering
    \includegraphics{pub/figures/physical_parameters_external_field.pdf}
    \caption{The physical properties of the anti-ferro-magnetic, para-magnetic %
        and ferro-magnetic systems as a function of the temperature and %
        for multiple magnetic field strengths. The anti-ferro-magnetic %
        material properties are plotted on the leftmost graph. The %
        para-magnetic material properties are plotted in the center and %
        the ferro-magnetic are on the right.}
    \label{fig:11}
\end{figure*}

Figure \ref{fig:11} is a very important figure so I will discuss it in %
a lot of detail. This figure evidences a lot of the qualitative discussion %
that I have undergone so far. Let's start by analysing the leftmost %
column which represents the anti-ferro-magnetic simulations. The most %
important result, which I will discuss first is the discontinuity in the %
heat capacity that occurs at approximately \(2.10 \epsilon / k\). This %
tells us that there is a second order phase transition. 


Moreover, comparing the plots for the entropy, energy, free energy and %
magnetisation to those in the middle column we can see that the %
anti-ferro-magnet begins to behave like the para-magnet. This confirms %
the prediction that the phase transition was from anti-ferro-magnetic %
to para-magnetic. Curiously both the energy and the entropy rise %
in the wrong order by magnetic field. The stronger magnetic field %
resulting in an earlier rise than the weaker. This does not match %
the para-magnetic behaviour although the asymptotes seem to be the same. 


We observe, once again as predicted, that the peak in the heat capacity %
shifts to lower temperatures as we increase the magnetic field. That said %
the effect is not noticeable for the first increase in the magnetic field %
strength which fails to noticeably shift the peak to the left. I also %
observed that the heat capacity seems to converge to a constant value %
on the hotter side of the peak. This matches the behaviour of the %
para-magnetic heat capacity. 


Now let's consider the middle column which represents the para-magnetic %
material. The most interesting thing about the para-magnetic material %
is that the high temperature behaviour matches the high temperature anti-%
ferro-magnetic behaviour. We observe that the heat capacity without an %
external magnetic field drops to zero. This makes sense as according to %
equation \ref{eqn:14} the energy will be constant. 


In particular the energy will be constant on average for an infinite %
lattice. There are clearly energy changes occurring all the time as %
spins flip agnostically up and down. However, the spins are exactly %
as likely to flip up as they are to flip down so the result is that %
the average works out to zero. The derivative of a constant is then %
of course zero. By the same reasoning the magnetisation and the energy %
are both zero. 


Adding the magnetic field causes the energy and the entropy to vary %
smoothly as a function of temperature. Examining equations \ref{eqn:12} %
and \ref{eqn:13} we see that the temperature dependence of the system %
is related to the quantity \(B / \tau\). Heuristically, this leads to %
the conclusion that the magnetic field and the temperature have %
complementary effects. That is if we increase the magnetic field strength %
we have to increase the temperature to maintain the same value of %
\(B / \tau\). 


I have already given a large amount of discussion to the behaviour of %
the ferro-magnetic Ising model so I will keep the analysis of the %
rightmost column brief. The interesting behaviour is invoked by the %
presence of the external magnetic field. In particular we see as predicted %
that the heat capacity peak is smoothed out and shifted to higher %
temperatures implying that the phase transition is now first order. %
This matches our predictions and confirms the presence of a positive %
feedback relation as opposed to an anti-ferro-magnetic material. 


Using equations \ref{eqn:14}, \ref{eqn:15}, \ref{eqn:16}, \ref{eqn:17} and %
\ref{eqn:18} we computed \(\chi^{2}\) statistics for the fitted model. %
To do this we used Poisson counting,
%
\begin{align}
    \chi^{2} &= \sum_{i}\left(\frac{o_{i} - e_{i}}{e_{i}}\right)^{2},
    \label{eqn:19}
\end{align}
%
where \(o_{i}\) is the observed data point in the \(i^{th}\) value and %
\(e_{i}\) is the expected value. This revealed what we would have expected %
based on the visual appearance of the data. The results of this analysis %
are included in table \ref{tab:1}.


\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|}
        \hline
        & \(B (T)\) & \(\chi^{2}\) & \(p\) \\
        \hline
        \hline
        \(U\)      & \(0\) & \(NAN\)                 & \(NAN\) \\ 
        \(\sigma\) & \(0\) & \(3.17 \times 10^{-5}\) & \(NAN\) \\
        \(F\)      & \(0\) & \(3.17 \times 10^{-5}\) & \(NAN\) \\
        \(C_{V}\)  & \(0\) & \(NAN\)                 & \(NAN\) \\
        \(|m|\)    & \(0\) & \(NAN\)                 & \(NAN\) \\
        \hline
        \(U\)      & \(1\) & \(5.34 \times 10^{-4}\) & \(<< 0.05\) \\ 
        \(\sigma\) & \(1\) & \(2.36 \times 10^{-3}\) & \(<< 0.05\) \\
        \(F\)      & \(1\) & \(1.99 \times 10^{-5}\) & \(<< 0.05\) \\
        \(C_{V}\)  & \(1\) & \(4.44 \times 10^{-1}\) & \(<< 0.05\) \\
        \(|m|\)    & \(1\) & \(5.34 \times 10^{-4}\) & \(<< 0.05\) \\
        \hline
        \(U\)      & \(2\) & \(4.95 \times 10^{-5}\) & \(<< 0.05\) \\ 
        \(\sigma\) & \(2\) & \(1.77\)                & \(0.00540\) \\
        \(F\)      & \(2\) & \(9.84 \times 10^{-5}\) & \(<< 0.05\) \\
        \(C_{V}\)  & \(2\) & \(80.2\)                & \(\sim 1\)  \\
        \(|m|\)    & \(2\) & \(4.95 \times 10^{-5}\) & \(<< 0.05\) \\
        \hline
    \end{tabular}
    \caption{\(\chi\) analysis of goodness of fit. I have included %
        three significant figures when quoting the \(\chi^{2}\) %
        statistic but it was used to maximum precision in calculations.}
    \label{tab:1}
\end{table}


Table \ref{tab:1} shows some very interesting trends. Firstly, we see %
that our model matches, to within a very good approximation, what we %
predicted in equations \ref{eqn:14}, \ref{eqn:15}, \ref{eqn:16} and %
\ref{eqn:17}. You may be suspicious of the \(NAN\), \(\chi^{2}\) values %
that I am reporting. You may notice that in these cases the predicted %
and model values are constant at zero in appropriate units. This %
resulted in divide by zero errors when I applied equation \ref{eqn:19}.


The next interesting element of table \ref{tab:1} is that the heat capacity %
performs much worse than the other physical parameters in terms of %
goodness of fit measured using the \(\chi^{2}\) analysis. I believe that %
this is because of how the heat capacity is measured, which from equation %
\ref{eqn:10} we can see uses the variance. For each parameter we run %
\(1000N^{2}\) trials with measurement and then average the result %
giving us extremely high accuracy. The heat capacity is the variance %
of these measurements and has consistently shown to converge slower than %
the mean (see \ref{fig:2} and \ref{fig:5}).


Although hard to see figure \ref{fig:11} contains error bars on all of the %
parameters except the heat capacity. We calculated the error in the heat %
capacity in the same way as other parameters using equation \ref{eqn:11}, %
but it was very large compared to the data and obscured the results on %
figure \ref{fig:11}. We proposed that better estimates for the heat %
capacity could be obtained by increasing the size of the lattice. We %
proposed this because it results in more runs and single flips cause %
smaller percentage changes in the energy.


To generate figure \ref{fig:11} we recorded data at ten evenly space %
points between zero and three \(\epsilon / k\) incremented by \(0.3 %
\epsilon / k\). As a result we used nine degrees of freedom to compute %
the \(p\)-values displayed in table \ref{tab:1}. The remaining unexplained %
aspects of the method were explain in the discussion of Question 1 b) and %
Question 2 b) respectively. The same method was applied here extended to %
account for the additional parameters.


I did not use figure \ref{fig:11} to estimate the Neel temperature %
because the samples were taken to broadly. Instead I wrote another %
program to measure the heat capacity over a much finer grid for a %
\(20 \times 20\) lattice. This program sampled temperatures from %
\(2.5 \epsilon / k\) to \(2.0 \epsilon / k\) in increments of %
\(0.025 \epsilon / k\). At each temperature the program ran for %
only \(100N^{2}\). I believe that lower number of steps maintains %
accuracy because the cooling is much slower. 


\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{pub/figures/heat_capacity_external_field.pdf}
    \caption{A finely gridded heat capacity measurement on a \(20 \times 20\) %
        lattice.}
    \label{fig:12}
\end{figure}


Figure \ref{fig:12} allows use to estimate the Neel temperature, but %
before we do it is worth discussing some of its features. Unlike figure %
\ref{fig:11} the heat capacity is included with error. This is because %
I ran more independent simulations, eight instead of five. Apparently %
even this small increase dramatically reduced the spread. Additionally, %
we observe that the curve, appears bumpy, when we know that minus %
the peak, the curve should be smooth. 


To estimate the Neel temperature I took the temperature corresponding %
to the greatest heat capacity, in this case \(T = 2.28 \pm 0.01\). %
The uncertainty that I have quoted for the Neel temperature is half %
the temperature increment rounded to one significant figure. %
Remarkably the Neel temperature is very similar to the Currie temperature. %
I believe that in our scenario this makes sense since the interactions %
strengths are the same and all that has changed is the sign. 


\section*{Conclusion}
I used a two dimensional Ising simulation to investigate the thermodynamic %
properties of anti-ferro-magnetic, para-magnetic and ferro-magnetic %
materials. With this simulation I was able to assess how well the %
para-magnetic simulation matched the theoretical models given by %
equations \ref{eqn:14}, \ref{eqn:15}, \ref{eqn:16} and \ref{eqn:17}. %
I found that the agreement was in general very good, but that the %
heat capacity failed to converge for the model in the strongest %
magnetic field. This was attributed to limitations of the simulation %
and not the theoretical model. I performed the goodness of fit tests %
using the \(\chi^{2}\) statistic and distribution.


I was also able to estimate the Neel temperature for a anti-ferro-magnetic %
Ising model. I did so graphically by plotting the output of my simulation %
and found that it was \(T = 2.28 \pm 0.01 \epsilon / k\), which was %
very similar to the Currie temperature for ferro-magnetic materials. %
The behaviour of the anti-ferro-magnetic, para-magnetic and ferro-magnetic %
material simulations is something that I discussed in detail. I observed %
that the anti-ferro-magnetic and ferro-magnetic materials displayed %
qualitatively similar behaviours, although the underlying spin distributions %
were markedly different. Among these behaviours was the formation of %
\emph{domains}, \emph{zones} and \emph{speckles}. 


\bibliographystyle{plain}
\bibliography{pub/citations}
\end{document}
