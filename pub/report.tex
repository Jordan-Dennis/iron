\documentclass[a4paper, twocolumn]{article}
\setlength{\textwidth}{180mm}
\setlength{\textheight}{250mm}
\setlength{\parindent}{0mm}
\setlength{\parskip}{2mm}
\setlength{\oddsidemargin}{15mm}
\setlength{\hoffset}{-1in}
\setlength{\topmargin}{-2.5pc}
\setlength{\headsep}{20pt}
\setlength{\columnsep}{4mm}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{listings}
\lstset{
    language=C, 
    literate={\ \ }{{\ }}1, 
    keywordstyle=\bfseries, 
    basicstyle=\ttfamily}
\def\ttdefault{pcr}
\def\citation{{\bfseries (Annon, dd/mm/yyyy)}}
\begin{document}
\section*{Foreword}
I used the \verb!c! programming language to complete this task. I %
represented the system of spins as a \verb!struct!, %
defined below in one dimension. 

\begin{lstlisting}
typedef struct ising_t
{
    float epsilon;
    float magnetic_field;
    float epsilon;
    int length;
    int *ensemble;
} ising_t;
\end{lstlisting}

Scaling this to two dimensions was more difficult because the %
\verb!*ensemble! had to be transformed into \verb!**ensemble!. %
It is possible to implement this in a single \verb!struct! %
using \verb!union ensemble {int *1d, int **2d};!, but in the %
code available on my \verb!github! I have used two separate %
\verb!struct!s resulting in code duplication for many of the %
methods. I made this decision because I had not used \verb!c! %
prior to this project and was unaware of the \verb!union! %
keyword and its uses. I am describing this because you may %
notice discrepancies in the code snippets that I have include %
where \verb!int *(*)ensemble = system -> ensemble;! is invoked %
to access the array of spins. 


\subsection*{Question 1 a)}
I selected three different temperatures, \(1.0, 2.0\) and %
\(3.0 \epsilon / k\), and ran the metropolis algorithm for %
\(1000N\) steps. At each of these temperatures the system %
was initialised in a random state and allowed to equilibrate. 

\begin{figure}[h]
    \includegraphics%
        [width=0.5\textwidth]%
        {pub/figures/first_and_last_ising_1d.pdf}
    \caption{Each vertical pair of lines %
        represents the spin state. The top one is %
        the random initial state and the bottom one %
        is the equilibrated final state.}
    \label{fig:1}
\end{figure}

We noticed that at lower temperatures the spin chunks were %
much larger than at higher temperatures. This is particularly %
pronounced between the \(T = 3.0\epsilon/k\) and \(T = %
1.0\epsilon/k\) plots in figure \ref{fig:1}. To evolve the %
system we used the version of the metropolis algorithm %
shown below, 

\begin{lstlisting}
/*
 * metropolis_step
 * ------------------------ 
 * Evolve the system by attempting to 
 * flip a spin. The step is weighted 
 * by the Boltzmann factor.
 *
 * parameters
 * ----------
 * ising_t *system: A struct 
 *     enscapulating the information 
 *     related to the system. 
 */
void metropolis_step(ising_t *system)
{
    float temp = system -> temperature;
    int *ensemble = system -> ensemble;
    int length = system -> length;
    int spin = random_index(length);
    int change = 2*ensemble[spin]*(
        ensemble[modulo(spin+1,length)] +
        ensemble[modulo(spin-1,length)]);

    if ((change < 0)
        || (exp(-change/temp)>randn()))
    {
        ensemble[spin] *= -1;
    }
}
\end{lstlisting}

where, \verb!rand! generates a random number in the range %
\([0, 1]\), and \verb!modulo! is modified to produce %
positive input on negative numbers like the \verb!python! %
implmentation. This is not the native \verb!c! implementation. %
I used the \verb!||! short circuit operator so that %
the second comparison was not evaluated on every call %
to the function. 
            

\subsection*{Question 1 b)}
I found that it was worth considering what the \emph{basic %
unit} of the Ising model was. In the absence of an external %
magnetic field the energy is a function of the pairs. I start %
by considering the partition function of an individual pair. %
This is a two level system; either the pair are aligned or they %
are anti-aligned with the corresponding energies.
%
\begin{align}
    Z_{i} &= \sum_{s_{i} = \pm 1}
            \exp\left(-\frac{\varepsilon s_{i}s_{i+1}}{\tau}\right)
            \nonumber\\
        &= \exp\left(-\frac{\varepsilon}{\tau}\right) +
            \exp\left(\frac{\varepsilon}{\tau}\right)
            \nonumber\\
        &= 2\cosh\left(\frac{\varepsilon}{\tau}\right).
    \label{eqn:1}
\end{align}


Similarly to the para-magnetic case we can multiply the system %
partition functions of single constituents together to get the %
partition function of the entire system. However, the condition %
to do this was that the constiutuents were independent, but the %
Ising model contains interactions. In the case of the Ising model %
the constituents that are independent are the pairs, not the %
individual spins. You may think think then that we only consider %
\(N / 2\) unique pairs but this is not the case. In a chain each %
spin is counted in two pairs so the power is still \(N\). 


A small detail that I skipped was what happens at the boundary. %
The two spins on the end of the chains are not (neccessarily) %
counted twice. In the limit of a very large chain of spins we %
can see that the boundary affect will not matter however, we %
got about this nuance in a much more interesting way by considering %
cyclic boundary conditions. That is to say that the spin on the %
far end of the chain is a neighbour to the spin at the start of the %
chain and vice versa. 


Given the partition function \(Z = (2\cosh(\varepsilon / \tau))^{N}\), we 
calculated the internal energy using,

\begin{align}
    U &= \tau^{2}\partial_{\tau}\ln(Z)\label{EQN1}\\
        &= \tau^{2}\partial_{\tau}
            \ln\left(2\cosh\left(\frac{\varepsilon}{\tau}\right)^{N}\right)
            \nonumber \\
        &= N\tau^{2}\partial_{\tau}
            \ln\left(2\cosh\left(\frac{\varepsilon}{\tau}\right)\right)
            \nonumber \\
        &= N\tau^{2}\partial_{\tau}
            \left(2\cosh\left(\frac{\varepsilon}{\tau}\right)\right)
            \frac{1}{2\cosh\left(\frac{\varepsilon}{\tau}\right)}
            \nonumber \\
        &= N\tau^{2}\partial_{\tau}\left(\frac{\varepsilon}{\tau}\right)
            \frac{\sinh\left(\frac{\varepsilon}{\tau}\right)}
            {\cosh\left(\frac{\varepsilon}{\tau}\right)}\nonumber \\
        &= -\varepsilon N\tanh\left(\frac{\varepsilon}{\tau}\right)
    \label{eqn:2}.
\end{align}

We calculated the free energy of the system using,

\begin{align}
    F &= -\tau\ln Z \\
        &= -\tau\ln\left(\left(
            2\cosh\left(\frac{\varepsilon}{\tau}\right)\right)^{N}\right) 
            \nonumber \\
        &= -N\tau\ln\left(
            2\cosh\left(\frac{\varepsilon}{\tau}\right)\right) 
            \nonumber \\
        &= -N\tau\ln\left(
            \exp\left(\frac{\varepsilon}{\tau}\right) + 
            \exp\left(-\frac{\varepsilon}{\tau}\right)\right) \nonumber \\
        &= -N\tau\ln\left(\exp\left(\frac{\varepsilon}{\tau}\right)
            \left(1 + \exp\left(-2\frac{\varepsilon}{\tau}\right)\right)
            \right)\nonumber \\
        &= -N\tau\ln\left(\exp\left(\frac{\varepsilon}{\tau}\right)\right)
            - N\tau\ln\left(1 + 
            \exp\left(-2\frac{\varepsilon}{\tau}\right)\right) \nonumber \\
        &= -N\varepsilon - N\tau\ln\left(1 + 
            \exp\left(-2\frac{\varepsilon}{\tau}\right)\right)
    \label{eqn:3}. 
\end{align}

The entropy followed from the combination of Equation \ref{eqn:2} and %
Equation \ref{eqn:1} using Equation \ref{eqn:3},

\begin{align}
    \tau\sigma &= F - U\\
        &= -N\varepsilon\tanh\left(\frac{\varepsilon}{\tau}\right) + 
            N\varepsilon + N\tau\ln\left(1 + 
            \exp\left(-2\frac{\varepsilon}{\tau}\right)\right)\nonumber \\
    \sigma &= \frac{\varepsilon}{\tau}\left(1 - 
            \tanh\left(\frac{\varepsilon}{\tau}\right)\right) +
            \ln\left(1 + \exp\left(-2\frac{\varepsilon}{\tau}\right)\right)
    \label{eqn:4}.
\end{align} 

Finally, we determined the specific heat using Equation \ref{eqn:2} %
and Equation \ref{eqn:3},

\begin{align}
    C &= \partial_{\tau}U\\
        &= \partial_{\tau}\left(-N\varepsilon\tanh\left(
            \frac{\varepsilon}{\tau}\right)\right)\nonumber\\
        &= -N\varepsilon\partial_{\tau}\left(\frac{\varepsilon}{\tau}\right)
            \frac{1}{\cosh^{2}\left(\frac{\varepsilon}{\tau}\right)}
            \nonumber\\
        &= \frac{N\varepsilon^{2}}{\tau^{2}\cosh^{2}\left(
            \frac{\varepsilon}{\tau}\right)}
    \label{eqn:5}.
\end{align}    


\subsection*{Question 1 c)}
\begin{figure}[h]
    \centering
    \includegraphics%
        [width=0.5\textwidth]%
        {pub/figures/physical_parameters_ising_1d.pdf}
    \caption{The top left is the energy and the %
        top right is the entropy. The bottom left is the %
        free energy and the bottom right is the heat capacity.}
    \label{fig:2}
\end{figure}        

I started by simulating a one dimensional Ising model with no %
external magnetic field, which I compared to the analytic %
expressions derived above. I used periodic boundary conditions %
and chose to implement my models using a lattice size of one-hundred %
spins. I chose to use one-hundred spins because it evaluated %
fast on my device and was large enough to be interesting. 


Starting with our one dimensional model we equilibrated the %
system for multiple different temperatures and settled on %
using \(1000N\) as the length of the loop. This was likely %
too many but I found that for low temperatures when the %
probability of a flip becomes small, a larger number of %
steps was required. 


I chose to sample the temperatures over the range \(0.0 - 4.0 %
\epsilon / k\) incrementing by \(0.2 \epsilon / k\). I %
initialised the system only once at the highest temperature %
that we sampled, \(3.8 \epsilon / k\). I equilibrated the %
system at this temperature by evolving it for \(1000N\) %
and then started to cool the system taking measurements %
at each new system. 


The alternative model was to randomly initialise the system %
at every temperature. This would require approximately %
twice the number of steps since the system would have to %
be equilibrated at every temperature. I realize that the %
cooling method has a side affect of leading to "overflow". %
By "overflow" I mean that the first few measurements of %
each temperature are slightly out of equilibrium at the %
higher temperature. 


The entire \emph{measured} cooling process was performed %
by the program \(1000\) times. At each temperature in the %
\emph{measured cooling} loop the energy and entropy %
were measured by taking the average of all \(1000 N\) %
iterations. The heat capacity was also measured by taking %
the variance of the energy and applying,
%
\begin{equation}
    C_{v} = \frac{\textrm{var}(\epsilon)}{\tau^{2}}
    \label{eqn:6}
\end{equation}


The energy and entropy were re-averaged over the outer %
\emph{fixed size} loop. I chose to re-average then on %
the outer loop because I was certain that the trials were %
statistically independent. This does not matter so much %
for the mean value estimate since the \(1000N\) inner %
loop allows the system to explore the equilibrium space %
but I think that it does matter for the error estimate. 


I belive that it matters for the error estimate because %
the state of the system is highly correlated to the %
past state of the system within some \emph{correlation %
length}. This \emph{correlation length} is roughly the %
same amount of time that the system requires to explore %
the equilibrium state, or \(1000N\).


By running multiple simulations for the \emph{correlation %
length} and averaging these results I have guaranteed %
robust results. I estimated the error using the \emph{standard %
error} of the independent \emph{correlation length} simulations. %
This means that the error presented in figure \ref{fig:2} is %
given by equation \ref{eqn:7}.
%
\begin{equation}
    \Delta\hat{\beta} = \sqrt{\frac{\textrm{var}(\beta)}{N}},
    \label{eqn:7}
\end{equation}
%
where, \(\hat{\beta}\) is the parameter estimate, \(\beta\) is a %
vector of measurements and \(N\) is the number of measurements. 


To calculate the energy of the system I used the following %
algorithm, 

\begin{lstlisting}
/*
 * energy_ising_t
 * --------------
 * Calculate the energy of the system.
 *
 * parameters
 * ----------
 * ising_t *system: A struct 
 *     enscapulating the information 
 *     related to the system. 
 *
 * returns
 * -------
 * float energy: The energy of the
 *     system in Joules. 
 */
float energy_ising_t(ising_t *system)
{
    int length = system->length;
    int *ensemble = system->ensemble;
    float energy = 0.;

    for(int spin=0; spin<length; spin++)
    {
        energy -= ensemble[spin] *  
            ensemble[modulo(spin+1,length)];
    }

    return energy;
}
\end{lstlisting}


You may notice that I am only counting the righthand neighbour %
of each spin. I chose this method because it is more optimal. %
If I was to count each neighbour then every pair would be %
counted twice and we would have to divide the final result by %
\verb!2.!. By only counting one of the neighbours I have %
halved the number of computations. 


A pair of spins is the \emph{base unit} of the ising model %
so to calculate the entropy I counted the number of aligned %
pairs and then used the \emph{chose} function to calculate %
the multiplicity. However, it was not quite this simples %
since \(100!\) is \(\sim 10^{157}\), which overflows an %
integer in the programs memory. 


To fix this problem I used the stirling approximation to %
compute the entropy directly. This implies that the %
entropy should be less accurate at lower temperatures %
where the entropy is low and the Stirling approximation %
diverges from the actual entropy. The code that I used %
to calculate the entropy was,

\begin{lstlisting}
/*
 * entropy_ising_t
 * ---------------
 * Calculate the entropy of a 
 * configuration. 
 *
 * parameters
 * ----------
 * ising_t *system: A struct 
 *     enscapulating the information 
 *     related to the system. 
 *
 * returns
 * -------
 * float entropy: The entropy of the
 *     system in natural units. 
 */
float entropy_ising_t(ising_t *system)
{ 
    int length = system->length;
    int *ensemble = system->ensemble; 
    int up = 0;

    for (int spin=0; spin<length; spin++)
    {
        up += ensemble[spin] == 
            ensemble[modulo(spin+1,length)];
    }

    int down = length - up;

    float entropy = length * log(length) - 
        up * log(up) - down * log(down);
    
    return entropy;   
}
\end{lstlisting}


\subsection*{Question 1 d)}
Consider the energy depicted in figure \ref{fig:2}. We see that %
the energy is a decreasing function of the temperature. This %
implies that the spins tend to align as the temperature decreases. %
This makes sense because the Boltzmann factor for the lower state %
becomes more favoured. Similarly the entropy is a increasing %
function of the temperature. As the spins tend to align at lower %
temperatures the number of ways to arrange the state becomes %
smaller, decreasing the entropy. 


The heat capacity, also shown %
in figure \ref{fig:2} is interesting because it increases %
rapidly before asymptotically decreasing. If it was discontinuous %
then we would expect a phase transition however it is continuous. %
so we know that there is no phase transition. We know it is continuous %
because of equation \ref{eqn:5}. 


\subsection*{Question 1 e)}
\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{pub/figures/magnetisation_ising_1d.pdf}
    \caption{Histograms of the magnetisation for two ising %
        systems of sizes \(N = 100\) and \(N = 500\). The %
        temperatures are labelled in the titles and the %
        \(y\)-axis is a unitless count.}
    \label{fig:3}
\end{figure}


Figure \ref{fig:3} showed me that the magnetisation was not zero %
as it was predicted to be at \(0 \epsilon / k\). I am not concerned %
because the distribution of samples, which roughly corresponds to %
a time average is symmetrically distributed around zero implying %
that the time average is identically zero. I would expect some %
variation of the results around zero as spins randomly flipped. %


Since magnetisation sets in at a lower temperature for the larger %
sample I concluded that there was no phase transition. I %
decided this because the phase transition should occur at the %
same temperature for all lattice sizes. Taking the limit of %
a decreasing function presumably reduces to zero implying that %
there is no spontaneous magnetisation for the one-dimensional %
Ising model. This means that there is no phase transition. 


\subsection*{Question 2 a)}
For \(\tau = 1.0 \epsilon / k\) the random lattice very quickly %
relaxed into large \emph{domains} of up and down spins. Over time %
the boundaries of the \emph{domains} moved around the lattice %
coherently. I am/will using/use \emph{coherent motion} to refer %
to a blob transporting across the network. For example, a group %
of down spins may start in the top right corner and remaining %
a single blob move to the middle of the system. 


I noticed that given enough time the \(\tau = 1.0 \epsilon / k\) lattice %
relaxed until it was entirely one color. This is not shown in figure %
\ref{fig:4} simply because the lattice was not allowed to evolve %
for enough time. I used the same evolution time for the two %
dimensional Ising model that I used for the one dimensional %
Ising model, that is \(1000N^{2}\) where I will use \(N\) to %
represent the side length of the system. 


This is important because at lower temperatures the system took longer %
to equilibrate. As I saw for the one-dimensional case the %
\emph{coherent domains} have a \emph{meta-stable} lifetime before decaying. %
The shape of this domain influences the lifetime. The size %
seems to be a direct predictor of lifetime with the largest %
domain nearly always coming out on top. Another important %
feature that we noticed was that the coherent domains became %
particularly \emph{meta-stable} if they formed rings. 


The ring structures were \emph{meta-stable} because of the cyclic %
boundary conditions. In a ring structure there are only two edges %
that are exposed to the other \emph{meta-stable} state meaning %
that the anti-aligned interactions at the boundary are minimised. %
Additionally since the interaction depth of the Ising model is %
only nearest neighbour the \emph{meta-stable} ring could exit %
in a band of just five spins. \emph{The spins on the boundary %
have no idea how large their respective domains are}.


A final point to address for the \(\tau = 1.0\epsilon / k\) model %
is the \emph{time speckling}. As the system is allowed to evolve %
in equilibrium spins at random locations will randomly flip. %
This is a byproduct of the metropolis algorithm but it is important %
because the \emph{speckles} almost always remain a single spin. %
You will see later that this changes as the temperature is increased. 


Heating the lattice to \(\tau = 2.0 \epsilon / k\) we see mush %
of the same behaviour. \(\tau = 2.0 \epsilon / k\) is still less %
than the critical temperature, so all of the same evolutionary %
events occur. \emph{Meta-stable} domains form, move and die. %
Even the more dangerous \emph{meta-stable} rings can form at %
this temperature. I would like to say that the lifetime of the %
\emph{domains} has declined but I have no evidence. 


The key difference between the \(\tau  2.0 \epsilon / k\) and the %
\(\tau = 1.0 \epsilon / k\) models is the \emph{speckling}. At %
the higher temperatures the speckles occur more often and can %
result in very local short lived domains. This is why I postulated %
that the \emph{domains} are shorter lived. The system is more %
active meaning that the spins on the boundary flip more readily. %
Moreover, the rapid speckling assists in the destruction of the %
\emph{domain} when it occurs on the boundary.


Something worthy of note before we move above the condensation %
temperature is that \emph{domains} die at there corners. This %
is because the corners represent an interface where a spin has %
more neighbours of one color than the other. This is another %
factor that prolongs the life of the \emph{meta-stable} states, %
since they have no corners and minimised contact. 


The behaviour was markedly different for the \(\tau = 3.0 \epsilon %
\ k\) model, because it is above the critical temperature. %
For starters the activity of the system has increased significantly. %
This means, among other things that the relaxation into equilibrium %
is almost instantaneous compared with the lower temperature models. %
In addition \emph{domains} do not form at this temperature. 


I have been using \emph{domain} to refer to a structure that is %
\emph{macroscopic} and \emph{semi-permanent}. \emph{Zonation} %
does occur at \(\tau = 3.0 \epsilon / k\) bur it does not meet %
these criteria. By this I mean that the zones are small \(\sim 10\) %
spins maximum and temporary. Rather than considering the rapid %
zonation to be the evolution of the \emph{domain} behaviour I %
consider it the evolution of the \emph{speckling} behaviour.


At \(\tau = 3.0 \epsilon / k\) we also loose the \emph{coherent} %
end state. At both of the other temperatures the final state was %
a completely magnetised block or a \emph{meta-stable} ring. %
However, at \(\tau = 3.0 \epsilon / k\) there is no such state %
and the \emph{zonation} appears to randomly move about the grid. %
At higher temperatures still the \emph{zonation} is completely %
destroyed and not even microscopic domains are able to form. 


\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{pub/figures/first_and_last_ising_2d.pdf}
    \caption{Snapshots of the Ising system at multiple temperatures %
        given a random initial starting position (top). You will %
        notice that \(\tau = 1.0 \epsilon / k\) and \(\tau = 2.0 \epsilon %
        /2\) have similar net magnetisation, but that \(\tau = 2.0 %
        \epsilon / k\) has stronger \emph{speckling} than \(\tau = %
        1.0 \epsilon / k\). You will notice that there is not net %
        magnetisation for \(\tau = 3.0 \epsilon / k\) and that the %
        \emph{zonation} I described above is occurring.}
    \label{fig:5}
\end{figure}


\subsection*{Question 2 b)}
Smoothly varying the temperature using I found that the \emph{speckling} %
and \emph{zonation} seems to dominate above \(\sim 2.3 \epsilon / k\) %
while below this \emph{domains} start to form and given enough time %
the entire crystal becomes magnetised. The transition is smooth %
with respect to the \emph{speckles}, which at very low temperatures %
are very small and isolated with out a lifetime. 


As I increased the temperature the \emph{speckles} increases in size and %
in lifetime. A \emph{speckle} is differentiated from a \emph{zone} rather %
arbitrarily, the two main differences being transport and lifetime. The %
\emph{zones} have a lifetime of long enough that they are able to %
\emph{coherently move} across the crystal. On the other hand a \emph{speckle} %
tends to die where it started and very quickly. 


As we approach the critical temperature from below the \emph{speckles} %
increase in size and disrupt the domains more quickly but, the system %
is still able to fully magnetise. At the critical temperature the %
\emph{speckles} become so disruptive that the magnetisation cannot occur %
macroscopically but it still occurs locally in the form of the \emph{zones} %
which outlive the now very active \emph{speckles}. 


Increasing the temperature further still the coherent \emph{zones} %
disappear (visually) entirely and the speckles become the dominant %
behaviour. As a final note it is worth noting that these behaviours %
I have described were visible to me in part because of the settings %
of my simulation. I found it was best to run them with many steps %
per frame \(5000 \sim 10000\) because it allows you to see the %
macroscopic behaviours. 


I acknowledge that although I wanted to use the \verb!qt! frame work %
to create my own \verb!gui! I ran out of time. Instead I used the %
free online applet created by Daniel V. Schroeder for Weber state %
university to make the higher level observations. The code that I used %
to evolve the two dimensional system was, 

\begin{lstlisting}
/*
 * metropolis_step_ising_t
 * -----------------------
 * Evolve the system according to a 
 * randomly weighted spin flip that 
 * compares the probability of the 
 * two states based on the Boltzmann 
 * distribution of the two systems. 
 *
 * parameters
 * ----------
 * ising_t *system: The system to 
 *     evolve. 
 */
void metropolis_step(ising_t *system)
{
    int length = system->length;
    int **ensemble = system->ensemble;
    float epsilon = system->epsilon;
    float temp = system->temperature;
    float field = system->magnetic_field;

    int row = random_index(length);
    int col = random_index(length);

    int spin = ensemble[row][col];
    int neighbours = 
        ensemble[modulo(row+1,length)][col] + 
        ensemble[modulo(row-1,length)][col] + 
        ensemble[row][modulo(col+1,length)] + 
        ensemble[row][modulo(col-1,length)];

    float magnetic_change=-2*spin*field;
    float interaction_change=2*epsilon*
        neighbours*spin;
    float change=magnetic_change+
        interaction_change;

    if ((energy_change < 0) || 
        (exp(-change/temperature) > randn())))
    {
        ensemble[row][col] *= -1;
    }
}
\end{lstlisting}


\textbf{Note}: The \verb!magnetic_field! was set to zero in the %
provided simulation and discussion, while the \verb!epsilon! %
was set to one. 


\subsection*{Question 2 c)}
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{pub/figures/physical_parameters_ising_2d.pdf}
    \caption{}
    \label{fig:7}
\end{figure}
I noticed that the heat capacity \emph{spike} increased in %
height as I increased the size of the crystal. This suggests %
that the infinite Ising model will have an infinitely tall %
heat capacity \emph{spike} at the critical temperature. I also %
noticed that at the critical temperature all of the physical %
parameters changed the fastest. 


This makes sense because a phase transition is loosely defined %
as a sudden discontinuity in the bulk properties of the sample. %
Although the entropy and energy are not discontinuous over the %
the critical temperature they do change very suddenly. The %
discontinuity is in the heat capacity which is the derivative %
of the energy with respect to temperature. Because a derivative %
is the discontinuous quantity it implies that the phase transition %
is second order. 


To measure the energy of the two dimensional Ising system I %
used a very similar piece of code to the one dimensional %
scenario. 

\begin{lstlisting}
/*
 * energy_ising_t
 * --------------
 * Calculate the energy of the 
 * Ising system. 
 *
 * parameters
 * ----------
 * ising_t *system: The system to 
 *     measure. 
 */
float energy(ising_t *system)
{
    int length = system->length;
    int **ensemble = system->ensemble;
    float epsilon = system->epsilon;
    float field = system->magnetic_field;
    float magnetic = 0.0;
    float interactions = 0.0;

    for (int row=0; row<length; row++)
    {
        for (int col=0; col<length; col++)
        {
            float neighbours = 
                ensemble[modulo(row+1,length)][col] +
                ensemble[modulo(row-1,length)][col] +
                ensemble[row][modulo(col+1,length)] +
                ensemble[row][modulo(col-1,length)];

            magnetic -= ensemble[row][col] * 
                field;
            interactions -= neighbours * 
                epsilon * ensemble[row][col];
        }
    }

    return interactions / 2. + magnetic;
}
\end{lstlisting}

For the entropy the calculation was also very similar. 

\begin{lstlisting}
/*
 * entropy_ising_t
 * --------------
 * Calculate the entropy of the Ising 
 * system. 
 *
 * parameters
 * ----------
 * ising_t *system: The system to 
 *     measure. 
 */
float entropy_ising_t(ising_t *system)
{
    int len = system->length;
    int **ensemble = system->ensemble; 
    int up = 0;

    for (int row=0; row<len; row++)
    {
        for (int col=0; col<len; col++)
        {
            up += ensemble[row][col] == 
                ensemble[modulo(row+1,len)][col];
            up += ensemble[row][col] == 
                ensemble[row][modulo(col+1,len)];
        }
    }
        
    int total = 2 * len * len;
    int down = total - up;

    return total * log(total) - 
        up * log(up) - down * log(down);
}
\end{lstlisting}


\subsection*{Question 2 d)}
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{pub/figures/magnetisation_ising_2d.pdf}
    \caption{The magnetisation of the Ising model as the %
        temperature and number of spins is varied.}
    \label{fig:8}
\end{figure}


I noticed that the magnetisation began to diverge at the same %
temperature irrespective of the size of the system. This is what %
I would expect from a phase transition because it should not depend %
on the size of the sample. To give a real world example iron melts %
at the same temperature regardless of the size of the block. %
We might more quickly melt a smaller block but that is just because %
there is less of it and less self insulation. 


As \(N\) increases the critical temperature will stay the same, %
but the steepness of the transition appears to radically increase. %
I was surprised to see this relationship and I believe it may %
be because we are at the very low number end of the spectrum. %
In this regime if one or two spins flip it becomes visible on %
the graph. What I expect to see is that the lines smoothly %
converge down to a step function at the critical temperature. 


\subsection*{Question 2 e)}
\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{pub/figures/heating_and_cooling_ising_2d.pdf}
    \caption{Heating and cooling an Ising system to change the %
        magnetisation.}
    \label{fig:9}
\end{figure}

I initialised the system randomly. This corresponds to the %
equilibrium of a system that is at a high temperature. %
Therefore, I first cooled the system to reach an equilibrium %
that I would expect for a low temperature state. This was done %
slowly letting the system equilibrate at each new temperature. %
This technique avoids the \emph{meta-stable} rings from forming. 






%\newpage
%\section*{Introduction}
%The study of magnetic materials is an area of academic %
%and industrial interest \citation. For example, magnetic %
%technologies are important in the ongoing development of %
%quantum computers, superconducting circuits and other %
%examples in electronics \citation. At a fundamental level %
%magnetisation is a well understood phenomenon, yet it is %
%difficult to theoretically model. One simple model of magnetic %
%materials is the Ising model. 
%
%
%The Ising model is the simplest model of a ferro-magnet \citation. %
%Despite the simplicity of the Ising model it displays rich %
%physical behaviour and has analytic solutions in one and %
%two dimensions \citation.  The Ising model is the simplest model %
%to account for inter-molecular interactions and contain a phase %
%transition. This makes it an excellent medium for studying %
%magnetic phenomenon \citation. 
%
%
%By modifying the basic Ising model we can simulate many %
%phenomenon including glasses \citation. The Ising model %
%has broader significance and can be used to construct very %
%simple neural networks called Boltzmann machines \citation. We tested %
%one and two dimension Ising models and confirmed that they %
%matched theoretical predictions.
%
%
%\section*{Theory}
%Materials have internal interactions. As physicists we like %
%to ignore these where possible but often these approximations %
%limit the accuracies of our models \citation. Magnetic %
%phenomenon are no different. To understand how spins interact %
%in a magnet it helps to first construct the simplest possible %
%model without interactions; a para-magnet.
%
%
%Consider our magnet as a one-dimensional chain of atomic spins. %
%For the moment ignore any external magnetic field and just %
%consider the spins in isolation. Now lets limit the spins to %
%be fixed up or down along one axis. If there are no interactions %
%between the spins the energy is fixed. If we add an external %
%magnetic field then we would expect the ensemble to develop a %
%net magnetisation.
%
%
%If the system has thermal energy we would expect some of the %
%spins to align themselves anti-parallel to the magnetic field. %
%We can see this affect by considering the partition function %
%for a single spin in the ensemble. If the spin is aligned with %
%the magnetic field then the energy is \(-sB\), where \(s\) is %
%the unit of magnetisation carried by the single spin and \(B\) %
%is the strength of the external magnetic field. If the spin is %
%anti-aligned with the field then the energy is \(sB\). 
%
%
%This is a simple two level system and the partition function %
%is given by, 
%%
%\begin{align}
%Z &= \sum_{s = \pm 1}\exp\left(-\frac{sB}{\tau}\right)\nonumber\\
%    &= \exp\left(-\frac{sB}{\tau}\right) + 
%        \exp\left(\frac{sB}{\tau}\right)\nonumber\\
%    &= 2\cosh\left(\frac{sB}{\tau}\right),
%\label{eqn:1}
%\end{align}
%%
%where, \(\tau = kT\) is the temperature in units of energy. %
%The probability of the spins being anti-aligned with the %
%field is therefore, 
%%
%\begin{align}
%P &= \frac{\exp\left(-\frac{sB}{\tau}\right)}
%        {2\cosh\left(\frac{sB}{\tau}\right)}.
%\label{eqn:2}
%\end{align}
%%
%Hence, as the temperature increase we expect the number of %
%anti-aligned spins to increase and as we increase the %
%magnetic field we expect the number of anti-aligned spins %
%to decrease. 
%
%
%Since each of the spins in a para-magnetic system is independent %
%the partition function of an ensemble of \(N\) spins is just %
%the product of \(N\) partition functions for the single spin %
%case. However, since the spins are indistinguishable we must %
%also divide by a Gibbs correction factor of \(N!\). %
%The probability of finding a particular state however, %
%is a case that is worth studying, since it indicates a %
%diveregence between the Ising model of a ferro-magnet and %
%a para-magnet in a magnetic field. First we need to define %
%our state. 
%
%
%The energy of the system, and any other physical %
%parameters, only depend on the number of spins that are %
%aligned with the magnetic field and not specifically %
%which spins are aligned with the field. Naively we might %
%expect that the probability of having \(N_{\uparrow}\) %
%spins aligned with the field would be,
%%
%\begin{align}
%P(N_{\uparrow}) &= \frac{
%        \exp\left(-\frac{sN_{\uparrow}B}{\tau}\right)
%        \exp\left(\frac{s(N - N_{\uparrow})B}{\tau}\right)}{
%        \cosh\left(\frac{sB}{\tau}\right)^{N}}.
%\label{eqn:3}
%\end{align}
%%
%However, equation \ref{eqn:3} has failed to account for the %
%multiple micro-states that occupy this macro-state. We can %
%account for this by multiplying by the multiplicity, which %
%can be found using the chose function,
%%
%\begin{align}
%P(N_{\uparrow}) &= \frac{N!}{N_{\uparrow}!(N - N_{\uparrow})!}
%        \frac{\exp\left(-\frac{sN_{\uparrow}B}{\tau}\right)
%        \exp\left(\frac{s(N - N_{\uparrow})B}{\tau}\right)}{
%        \cosh\left(\frac{sB}{\tau}\right)^{N}}.
%\label{eqn:4}
%\end{align}
%%
%Equation \ref{eqn:4} is the correct expression for the probability.
%
%
%It is informative to calculate the internal energy and free %
%energy of the system. Starting with the internal energy,
%%
%\begin{align}
%U &= \tau^{2}\partial_{\tau}\ln Z\nonumber\\
%    &= \tau^{2}\partial_{\tau}\ln\left(2^{N}\cosh^{N}
%        \left(\frac{sB}{\tau}\right)\right)\nonumber\\
%    &= -NsB\tanh\left(\frac{sB}{\tau}\right).
%\label{eqn:5}
%\end{align}
%%
%We can also calculate the free energy, but further calculations %
%result in tedious analytical expressions so we have omitted them. %
%%
%\begin{align}
%F &= -\tau\ln Z\nonumber\\
%    &= -\tau\ln\left(2^{N}\cosh^{N}\left(\frac{sB}{\tau}
%        \right)\right)\nonumber\\
%    &= -NsB - N\tau\ln\left(1 + \exp\left(-\frac{2sB}{\tau}\right)
%        \right).
%\label{eqn:7}
%\end{align}
%%
%As we will see when we analyse the Ising model without an external %
%field these results are general of any two level system. Using %
%equation \ref{eqn:5} we can calculate the magnetisation as a function %
%of the magnetic field and temperature, 
%%
%\begin{align}
%U = mB &= -NsB\tanh\left(\frac{sB}{\tau}\right)\nonumber\\
%    m &= -Ns\tanh\left(\frac{sB}{\tau}\right).
%\label{eqn:8}
%\end{align}
%%
%Therefore, the net magnetisation system will decrease with %
%temperature and increase with the magnetic field, much as we %
%would expect. 
%
%
%Para-magnets are a useful toy model but from our experience %
%with natural and manufactured magnets we know that it is %
%possibe to construct systems that are magnetic without external %
%fields. The one-dimensional Ising model is a simple model of %
%such systems. The Ising model is a natural extension of the %
%paramagnetic model that we discussed, and operates on the same %
%spin lattice. 
%
%
%The Ising model differs because it adds very simple interactions %
%between neighbouring spins. This interaction favours pairs that %
%are aligned by reducing the energy of this scenario. Representing %
%up spins as \(+1\) and down spins as \(-1\) we can represent this %
%mutal interaction as \(\Delta \epsilon = \varepsilon s_{i}s_{i + 1}\), %
%where \(\Delta \epsilon\) is the energy contribution of the %
%interaction, \(\varepsilon\) is a scaling factor that represents %
%the strength of the interaction and \(s_{i}\) is the \(i^{th}\) %
%spin in the chain. 
%
%
%
%
%What happens if we place the Ising model into an external magnetic %
%field. Again we can break it down by considering a single pair %
%in the chain as our constituent object. There are three energies %
%that it is possible for this pair to have; parallel and aligned %
%with the magnetic field, parallel and anti-aligned with the %
%magnetic field and anti-parallel. However, the final state has a %
%multiplicity of two since either of the spins could be aligned %
%with the field. 
%
%
%It is possible to compute the partition function for this the single %
%pair and hence also the entrie system.
%%
%\begin{align}
%Z_{1} &= \sum_{s}\exp\left(-\frac{\epsilon_{s}}{\tau}\right)
%        \nonumber\\
%    &= \exp\left(\frac{-\epsilon - 2B}{\tau}\right) +
%        2\exp\left(\frac{\epsilon}{\tau}\right) +
%        \exp\left(\frac{-\epsilon + 2B}{\tau}\right)
%        \nonumber\\
%    &= 2\exp\left(\frac{-\epsilon}{\tau}\right)
%        \cosh\left(\frac{2B}{\tau}\right) + 
%        2\exp\left(\frac{\epsilon}{\tau}\right).
%\label{eqn:14}
%\end{align}
%%
%From here we can compute all the physical properties of the system. %
%Since each pair is independent the partition function is simply the %
%product of \(N\) partition function by the same reasoning as in %
%the case above when there was no magnetic field. We have not shown %
%the calculation of the energy ect., because the expressions are %
%complex and uninformative. 
%
%
%Another interesting effect that can be explored using the Ising model %
%is anti-ferro-magnetism. This phenomenon was only recently discovered %
%in nature \citation and refers to and interaction between neighbouring %
%spins that causes them to have lower energy when they are aligned %
%anti-parallel rather than parallel. We do not need to cover any new %
%equations in this case as an anti-ferro-magnet can be explored by %
%letting \(\epsilon\) become negative. 
%
%
%We have spent a lot of time discussing the one-dimensional scenario %
%but real systems are typically higher dimensional. There is a %
%an analytical solution to the two-dimensional ising model \citation. %
%This solution is a tour de force and has comparatively little %
%practical use due to its complexity. Multiple approximation methods %
%have been developed for dealing with the two-dimensional case, %
%most notably the mean field approximation. 
%
%
%The mean field approximation treats a group of neighbours as an %
%a single spin, parametrised by the mean. In this way we recover %
%the two level system and arrive at a two level system that is %
%very similar to what we have already covered for the para-magnet %
%and the Ising model when there is no external magnetic field. %
%Ultimately the mean field approximation is an approximation and %
%its predictions are not always correct. 
%
%
%Another aspect of higher dimensions that it is worth discussing %
%is what counts as a neighbour. For example, in two-dimensions %
%we could connet the spins together so that each spins is equally %
%far from six other spins. This triangular Ising model will have %
%markedly different behaviour than a square grid of spins \citation. %
%For our analysis we have considered a square grid of spins since %
%it is simpler to simulate. 
%
%
%Without going into the complex analytical solutions we can still %
%make useful qualitative guesses about the behaviour of the Ising %
%model in higher dimensions based on its behaviour in lower %
%dimensions. For example, we expect the spins will tend to align %
%at lower temperatures and tend to disorder at higher temperatures. %
%For the anti-ferro-magnetic case we expect the spins to become %
%anti-aligned at low temperatures and tend to disorder at higher %
%temperatures.
%
%
%It is worth noting that in the presence of a magnetic field the %
%qualitative behaviour of the anti-ferro-magnetic and ferro-magnetic %
%Ising models becomes markedly different. The ferro-magnetic model %
%results in a positive feedback loop as the magnetic field coerces %
%spins to align with the field they also want to align with each other.
%On the other hand the anti-ferro-magnet exerts a dampening effect %
%for the opposite reason. 
%
%
%\section*{Method}
%\begin{verbatim}
%function calc_energy
%energy = 0
%for spin in 0:length
%energy += ensemble[spin] * ensemble[spin + 1]
%return energy
%\end{verbatim}
%
%
%Similarly for the entropy we counted all of the aligned pairs. %
%This works because as discussed in the theory the "base unit" %
%of the Ising model is a pair of spins not an individual spin. %
%As noted in the Theory there are \(N\) pairs of spins. Moreover, %
%we ued Stirling's approximation in the logarithmic form, because %
%we found that the program could not calculate numbers of the size %
%\(100!\). This will have had negligible affects at higher %
%temperatures where the system tends torward disorder, but at %
%lower temperatures the approximation becomes less accurate. %
%We were not too concerned with the loss of accuracy since the %
%entropy tends to zero at low temperatures.
%%
%\begin{verbatim}
%function calc_entropy
%up = 0
%for spin in 0:length
%up += ensemble[spin] == ensemble[spin + 1]
%down = length - up
%entropy = length * log(length) - up * log(up) - down * log(down)
%return entropy
%\end{verbatim}
%
%
%The free energy was calculated using its definition, \(F = U - %
%\tau\sigma\), where \(\tau = kT\) and \(\sigma\) is the entropy. %
%The heat capacity was calculated using the thermodynamic %
%identity,
%%
%\begin{align}
%C_{V} &= \frac{\textrm{var(U)}{\tau^{2}}}.
%\label{eqn:15}
%\end{align}
%%
%A large part of the computational expense came from estimating the %
%uncertainty in the physical parameters. It did not make sense to %
%initialise the system randomly at every temperature and then wait %
%for it to equilibrate before taking measurements. Instead we %
%initialised the system at a high temperature where the random %
%configuration is a good approximation to the equilibrium %
%configuration and equilibrated it. 
%
%
%From this higher temperature %
%we allowed the system to evolve for \(1000N\) steps measuring %
%the physical properties at every step. We then cooled the system %
%by a small incremement and without re-equilibrating the system %
%evolved it for \(1000N\) steps taking measurements every step. %
%In this way we halved the amount of CPU time required by each run %
%but introduced a small error by starting the system at a slightly %
%out of equilibrium state. Given the large number of runs we believe %
%that this error is negligible, although it is visible at lower %
%temperatures as the heat capacity becomes over-estiamted. 
%
%
%Another flaw of running the simulation this way was that it %
%serialised the loops (i.e. made the next iteration depend on the %
%state of the previous one). This prevented us from making efficient %
%parallelisation of the inner loop, however, the outer loop was %
%not serial and could be efficiently paralellised. After each run %
%of \(1000N\) the average was taken for each physical parameter. 
%
%
%
%Please note that we did not use the standard error of the %
%\(1000N\) trials as the error. It does not make sense to do %
%so because each state is deterministically dependent on the %
%previous one and the sample gridding is much too fine. Instead %
%we repeated the entire process a fixed number of times \(100\) for %
%the one-dimensional case and used the standard error of the means %
%from these 100 trials as the estimate of our uncertainty. Note that %
%we defined the standard error as \(\sqrt{\textrm{Var}{N}}\). %
%
%
%We noticed that the infinite one-dimensional Ising model is predicted %
%to have no net magnetisation at \(0K\). To test this hypothesis, %
%we created histograms of the magnetisation at \(\tau = 0.5J, 1.0J\) %
%and \(2.0J\) for \(N = 100\) and \(N = 500\). We expected the %
%histograms to be narrower as the number of spins in the system was %
%increased.
%
%
%Once we were satisfied with the one-dimensional ising model we %
%repeated a similar analysis for the two-dimensional model. %
%First we tested the time required for equilibration by initialising %
%the model and running it for \(1000N^{2}\) where \(N\) is the width %
%of the grid. 
%
%
%Satisfied with the equilibration time we employed the same techniques %
%described for the one-dimensional case to the two-dimensional case %
%to measure the physical parameters. In short, we incrementally cooled %
%the system measuring every iteration and recording the mean. We %
%repeated this process a fixed number of times and used the standard %
%error as our uncertainty estimate. The energy calculation is %
%clearly modified, so we have included the relevant pseudocode %
%below:
%%
%
%
%\section*{Results}
%We noticed that the net magnetisation set in at lower temperatures for %
%the larger \(N\). This tells us that there is no phase transition %
%because a phase transition should occur at exactly the same %
%temperature for all latice sizes. Moreover, it agrees with the %
%theoretical prediction that the Ising model is not magnetised at %
%\(0K\) in the infinite case because it we increase the latticed %
%temperature to infinity then the temperature of net magnetisation %
%should decrease below \(0K\). 
\end{document}
